{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm-emb.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCVLM1zGlDyT"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKXkJxtElNnC"
      },
      "source": [
        "from matplotlib import rcParams, pyplot as plt\n",
        "import numpy as np\n",
        "import string\n",
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import re\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from tensorflow.keras import Sequential\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, GlobalMaxPooling1D, Conv1D, Dropout, Bidirectional, concatenate\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import plot_model, to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import warnings \n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBuoAKLAlPtX",
        "outputId": "109cef71-d428-44e5-ccea-e12c60ea0d13"
      },
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    # Restrict TensorFlow to only use the first GPU\n",
        "    try:\n",
        "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "    except RuntimeError as e:\n",
        "        # Visible devices must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "else:\n",
        "    print('No GPU detected')\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 Physical GPUs, 1 Logical GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jjkmCowlUDE"
      },
      "source": [
        "rcParams['figure.figsize'] = (16, 8)\n",
        "plt.style.use('fivethirtyeight')\n",
        "pd.set_option('max_columns', 100)\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "warnings.simplefilter('ignore')"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozqcZT2llXxW"
      },
      "source": [
        "data_dir = Path('/content/drive/MyDrive/dacon/input')\n",
        "feature_dir = Path('../build/feature')\n",
        "val_dir = Path('/content/drive/MyDrive/dacon/build/val')\n",
        "tst_dir = Path('/content/drive/MyDrive/dacon/build/tst')\n",
        "sub_dir = Path('/content/drive/MyDrive/dacon/build/sub')\n",
        "\n",
        "trn_file = data_dir / 'train.csv'\n",
        "tst_file = data_dir / 'test_x.csv'\n",
        "sample_file = data_dir / 'sample_submission.csv'\n",
        "\n",
        "target_col = 'author'\n",
        "n_fold = 5\n",
        "n_class = 5\n",
        "seed = 42"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wedaBd8lZWp"
      },
      "source": [
        "algo_name = 'lstm'\n",
        "feature_name = 'emb'\n",
        "model_name = f'{algo_name}_{feature_name}'\n",
        "\n",
        "feature_file = feature_dir / f'{feature_name}.csv'\n",
        "p_val_file = val_dir / f'{model_name}.val.csv'\n",
        "p_tst_file = tst_dir / f'{model_name}.tst.csv'\n",
        "sub_file = sub_dir / f'{model_name}.csv'"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "q_ytTVt1lbmr",
        "outputId": "501d6034-6bc4-41c1-f613-ec423ec7340b"
      },
      "source": [
        "train = pd.read_csv(trn_file, index_col=0)\n",
        "train.head()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>He was almost choking. There was so much, so m...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>“Your sister asked for it, I suppose?”</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>She was engaged one day as she walked, in per...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The captain was in the porch, keeping himself ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  author\n",
              "index                                                           \n",
              "0      He was almost choking. There was so much, so m...       3\n",
              "1                 “Your sister asked for it, I suppose?”       2\n",
              "2       She was engaged one day as she walked, in per...       1\n",
              "3      The captain was in the porch, keeping himself ...       4\n",
              "4      “Have mercy, gentlemen!” odin flung up his han...       3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "wT6B7qc9ldzb",
        "outputId": "3e9b013f-77b5-4c40-aca4-215828c2300a"
      },
      "source": [
        "test = pd.read_csv(tst_file, index_col=0)\n",
        "test.head()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>“Not at all. I think she is one of the most ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"No,\" replied he, with sudden consciousness, \"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>As the lady had stated her intention of scream...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>“And then suddenly in the silence I heard a so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>His conviction remained unchanged. So far as I...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text\n",
              "index                                                   \n",
              "0      “Not at all. I think she is one of the most ch...\n",
              "1      \"No,\" replied he, with sudden consciousness, \"...\n",
              "2      As the lady had stated her intention of scream...\n",
              "3      “And then suddenly in the silence I heard a so...\n",
              "4      His conviction remained unchanged. So far as I..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdXtBAEjljkq"
      },
      "source": [
        "train['text'] = train['text'].str.replace('\\?',' quesmark ')\n",
        "train['text'] = train['text'].str.replace('\\!',' exclmark ')\n",
        "train['text'] = train['text'].str.replace('\\&',' empent ')\n",
        "train['text'] = train['text'].str.replace(\"\\*\",' chstar ')\n",
        "train['text'] = train['text'].str.replace(\";\",' smcolons  ')\n",
        "\n",
        "test['text'] = test['text'].str.replace('\\?',' quesmark ')\n",
        "test['text'] = test['text'].str.replace('\\!',' exclmark ')\n",
        "test['text'] = test['text'].str.replace('\\&',' empent ')\n",
        "test['text'] = test['text'].str.replace(\"\\*\",' chstar ')\n",
        "test['text'] = test['text'].str.replace(\";\",' smcolons  ')"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0iv9cqallDZ"
      },
      "source": [
        "cont_dict={\"ain't\": 'are not',\n",
        " \"aren't\": 'are not',\n",
        " \"can't\": 'can not',\n",
        " \"can't've\": 'can not have',\n",
        " \"'cause\": 'because',\n",
        " \"could've\": 'could have',\n",
        " \"couldn't\": 'could not',\n",
        " \"couldn't've\": 'could not have',\n",
        " \"didn't\": 'did not',\n",
        " \"doesn't\": 'does not',\n",
        " \"don't\": 'do not',\n",
        " \"hadn't\": 'had not',\n",
        " \"hadn't've\": 'had not have',\n",
        " \"hasn't\": 'has not',\n",
        " \"haven't\": 'have not',\n",
        " \"\\'he'd\": 'he would',\n",
        " \"\\'he'd've\": 'he would have',\n",
        " \"\\'he'll\": 'he will',\n",
        " \"\\'he'll've\": 'he will have',\n",
        " \"\\'he's\": 'he is',\n",
        " \"\\'how'd\": 'how did',\n",
        " \"\\'how're\": 'how are',\n",
        " \"\\'how'd'y\": 'how do you',\n",
        " \"\\'how'll\": 'how will',\n",
        " \"\\'how's\": 'how is',\n",
        " \"\\'I'd\": 'I would',\n",
        " \"\\'I'd've\": 'I would have',\n",
        " \"\\'I'll\": 'I will',\n",
        " \"\\'I'll've\": 'I will have',\n",
        " \"\\'I'm\": 'I am',\n",
        " \"\\'I've\": 'I have',\n",
        " \"\\'he'd\": 'he would',\n",
        " \"he'd've\": 'he would have',\n",
        " \"he'll\": 'he will',\n",
        " \"he'll've\": 'he will have',\n",
        " \"he's\": 'he is',\n",
        " \"how'd\": 'how did',\n",
        " \"how're\": 'how are',\n",
        " \"how'd'y\": 'how do you',\n",
        " \"how'll\": 'how will',\n",
        " \"how's\": 'how is',\n",
        " \"I'd\": 'I would',\n",
        " \"I'd've\": 'I would have',\n",
        " \"I'll\": 'I will',\n",
        " \"I'll've\": 'I will have',\n",
        " \"I'm\": 'I am',\n",
        " \"I've\": 'I have',         \n",
        " \"isn't\": 'is not',\n",
        " \"\\'it'd\": 'it would',\n",
        " \"\\'it'd've\": 'it would have',\n",
        " \"\\'it'll\": 'it will',\n",
        " \"\\'it'll've\": 'it will have',\n",
        " \"\\'it's\": 'it is',\n",
        " \"\\'let's\": 'let us',\n",
        " \"it'd\": 'it would',\n",
        " \"it'd've\": 'it would have',\n",
        " \"it'll\": 'it will',\n",
        " \"it'll've\": 'it will have',\n",
        " \"it's\": 'it is',\n",
        " \"let's\": 'let us',\n",
        " \"ma'am\": 'madam',\n",
        " \"mayn't\": 'may not',\n",
        " \"might've\": 'might have',\n",
        " \"mightn't\": 'might not',\n",
        " \"mightn't've\": 'might not have',\n",
        " \"must've\": 'must have',\n",
        " \"mustn't\": 'must not',\n",
        " \"mustn't've\": 'must not have',\n",
        " \"needn't\": 'need not',\n",
        " \"needn't've\": 'need not have',\n",
        " \"o'clock\": 'of the clock',\n",
        " \"oughtn't\": 'ought not',\n",
        " \"oughtn't've\": 'ought not have',\n",
        " \"\\'shan't\": 'shall not',\n",
        " \"\\'sha'n't\": 'shall not',\n",
        " \"\\'shan't've\": 'shall not have',\n",
        " \"\\'she'd\": 'she would',\n",
        " \"\\'she'd've\": 'she would have',\n",
        " \"\\'she'll\": 'she will',\n",
        " \"\\'she'll've\": 'she will have',\n",
        " \"\\'she's\": 'she is',\n",
        " \"\\'should've\": 'should have',\n",
        " \"\\'shouldn't\": 'should not',\n",
        " \"\\'shouldn't've\": 'should not have',\n",
        " \"shan't\": 'shall not',\n",
        " \"sha'n't\": 'shall not',\n",
        " \"shan't've\": 'shall not have',\n",
        " \"she'd\": 'she would',\n",
        " \"she'd've\": 'she would have',\n",
        " \"she'll\": 'she will',\n",
        " \"she'll've\": 'she will have',\n",
        " \"she's\": 'she is',\n",
        " \"should've\": 'should have',\n",
        " \"shouldn't\": 'should not',\n",
        " \"shouldn't've\": 'should not have',         \n",
        " \"so've\": 'so have',\n",
        " \"so's\": 'so is',\n",
        " \"\\'that'd\": 'that would',\n",
        " \"\\'that'd've\": 'that would have',\n",
        " \"\\'that's\": 'that is',\n",
        " \"\\'there'd\": 'there would',\n",
        " \"\\'there'd've\": 'there would have',\n",
        " \"\\'there's\": 'there is',\n",
        " \"\\'they'd\": 'they would',\n",
        " \"\\'they'd've\": 'they would have',\n",
        " \"\\'they'll\": 'they will',\n",
        " \"\\'they'll've\": 'they will have',\n",
        " \"\\'they're\": 'they are',\n",
        " \"\\'they've\": 'they have',\n",
        " \"that'd\": 'that would',\n",
        " \"that'd've\": 'that would have',\n",
        " \"that's\": 'that is',\n",
        " \"there'd\": 'there would',\n",
        " \"there'd've\": 'there would have',\n",
        " \"there's\": 'there is',\n",
        " \"they'd\": 'they would',\n",
        " \"they'd've\": 'they would have',\n",
        " \"they'll\": 'they will',\n",
        " \"they'll've\": 'they will have',\n",
        " \"they're\": 'they are',\n",
        " \"they've\": 'they have',         \n",
        " \"to've\": 'to have',\n",
        " \"wasn't\": 'was not',\n",
        " \"\\'we'd\": 'we would',\n",
        " \"\\'we'd've\": 'we would have',\n",
        " \"\\'we'll\": 'we will',\n",
        " \"\\'we'll've\": 'we will have',\n",
        " \"\\'we're\": 'we are',\n",
        " \"\\'we've\": 'we have',\n",
        " \"we'd\": 'we would',\n",
        " \"we'd've\": 'we would have',\n",
        " \"we'll\": 'we will',\n",
        " \"we'll've\": 'we will have',\n",
        " \"we're\": 'we are',\n",
        " \"we've\": 'we have',\n",
        " \"weren't\": 'were not',\n",
        " \"\\'what'll\": 'what will',\n",
        " \"\\'what'll've\": 'what will have',\n",
        " \"\\'what're\": 'what are',\n",
        " \"\\'what's\": 'what is',\n",
        " \"\\'what've\": 'what have',\n",
        " \"\\'when's\": 'when is',\n",
        " \"\\'when've\": 'when have',\n",
        " \"\\'where'd\": 'where did',\n",
        " \"\\'where's\": 'where is',\n",
        " \"\\'where've\": 'where have',\n",
        " \"\\'who'll\": 'who will',\n",
        " \"\\'who'll've\": 'who will have',\n",
        " \"\\'who's\": 'who is',\n",
        " \"\\'who've\": 'who have',\n",
        " \"\\'why's\": 'why is',\n",
        " \"\\'why've\": 'why have',\n",
        " \"\\'will've\": 'will have',\n",
        " \"\\'won't\": 'will not',\n",
        " \"\\'won't've\": 'will not have',\n",
        " \"\\'would've\": 'would have',\n",
        " \"\\'wouldn't\": 'would not',\n",
        " \"\\'wouldn't've\": 'would not have',\n",
        " \"what'll\": 'what will',\n",
        " \"what'll've\": 'what will have',\n",
        " \"what're\": 'what are',\n",
        " \"what's\": 'what is',\n",
        " \"what've\": 'what have',\n",
        " \"when's\": 'when is',\n",
        " \"when've\": 'when have',\n",
        " \"where'd\": 'where did',\n",
        " \"where's\": 'where is',\n",
        " \"where've\": 'where have',\n",
        " \"who'll\": 'who will',\n",
        " \"who'll've\": 'who will have',\n",
        " \"who's\": 'who is',\n",
        " \"who've\": 'who have',\n",
        " \"why's\": 'why is',\n",
        " \"why've\": 'why have',\n",
        " \"will've\": 'will have',\n",
        " \"won't\": 'will not',\n",
        " \"won't've\": 'will not have',\n",
        " \"would've\": 'would have',\n",
        " \"wouldn't\": 'would not',\n",
        " \"wouldn't've\": 'would not have',\n",
        " \"y'all\": 'you all',\n",
        " \"y'all'd\": 'you all would',\n",
        " \"y'all'd've\": 'you all would have',\n",
        " \"y'all're\": 'you all are',\n",
        " \"y'all've\": 'you all have',\n",
        " \"\\'you'd\": 'you would',\n",
        " \"\\'you'd've\": 'you would have',\n",
        " \"\\'you'll\": 'you will',\n",
        " \"\\'you'll've\": 'you shall have',\n",
        " \"\\'you're\": 'you are',\n",
        " \"\\'you've\": 'you have',\n",
        " \"you'd\": 'you would',\n",
        " \"you'd've\": 'you would have',\n",
        " \"you'll\": 'you will',\n",
        " \"you'll've\": 'you shall have',\n",
        " \"you're\": 'you are',\n",
        " \"you've\": 'you have',\n",
        " 'jan.': 'january',\n",
        " 'feb.': 'february',\n",
        " 'mar.': 'march',\n",
        " 'apr.': 'april',\n",
        " 'jun.': 'june',\n",
        " 'jul.': 'july',\n",
        " 'aug.': 'august',\n",
        " 'sep.': 'september',\n",
        " 'oct.': 'october',\n",
        " 'nov.': 'november',\n",
        " 'dec.': 'december',\n",
        " 'ain’t': 'are not',\n",
        " 'aren’t': 'are not',\n",
        " 'can’t': 'can not',\n",
        " 'can’t’ve': 'can not have',\n",
        " '’cause': 'because',\n",
        " 'could’ve': 'could have',\n",
        " 'couldn’t': 'could not',\n",
        " 'couldn’t’ve': 'could not have',\n",
        " 'didn’t': 'did not',\n",
        " 'doesn’t': 'does not',\n",
        " 'don’t': 'do not',\n",
        " 'hadn’t': 'had not',\n",
        " 'hadn’t’ve': 'had not have',\n",
        " 'hasn’t': 'has not',\n",
        " 'haven’t': 'have not',\n",
        " '\\'he’d': 'he would',\n",
        " '\\'he’d’ve': 'he would have',\n",
        " '\\'he’ll': 'he will',\n",
        " '\\'he’ll’ve': 'he will have',\n",
        " '\\'he’s': 'he is',\n",
        " '\\'how’d': 'how did',\n",
        " '\\'how’re': 'how are',\n",
        " '\\'how’d’y': 'how do you',\n",
        " '\\'how’ll': 'how will',\n",
        " '\\'how’s': 'how is',\n",
        " '\\'I’d': 'I would',\n",
        " '\\'I’d’ve': 'I would have',\n",
        " '\\'I’ll': 'I will',\n",
        " '\\'I’ll’ve': 'I will have',\n",
        " '\\'I’m': 'I am',\n",
        " '\\'I’ve': 'I have',\n",
        " '\\'isn’t': 'is not',\n",
        " '\\'it’d': 'it would',\n",
        " '\\'it’d’ve': 'it would have',\n",
        " '\\'it’ll': 'it will',\n",
        " '\\'it’ll’ve': 'it will have',\n",
        " '\\'it’s': 'it is',\n",
        " '\\'let’s': 'let us',  \n",
        " 'he’d': 'he would',\n",
        " 'he’d’ve': 'he would have',\n",
        " 'she’ll': 'he will',\n",
        " 'he’ll’ve': 'he will have',\n",
        " 'odin’s' : 'odin is',\n",
        " 'joe’s' : 'joe is',\n",
        " 'dora’s' : 'dora is',\n",
        " 'wickfield’s' : 'wickfield is',\n",
        " 'tellson’s' : 'tellson is',\n",
        " 'omer’s' :  'omer is',\n",
        " 'cruncher’s' : 'crucher is', \n",
        " 'pip’s' : 'pip is',\n",
        " 'creakle’s ': 'creakle is',\n",
        " 'jorkins’s ' : 'jorkins is',\n",
        " 'jane’s' : 'jane is',\n",
        " 'elliot’s' : 'elliot is',\n",
        " 'anne’s' : 'anne is',\n",
        " 'tilney’s' : 'tilney is',\n",
        " 'lizzy’s' : 'lizzy is',\n",
        " 'smith’s' : 'smith is',\n",
        " 'walter’s' : 'walter is',\n",
        " 'musgrove’s' : 'musgrove is',\n",
        " 'lucy’s' : 'lucy is',\n",
        " 'nigel’s' : 'nigel is',\n",
        " 'nay’s' : 'nay is',\n",
        " 'chodinger’s' : 'chodinger is',\n",
        " 'humphrey’s' : 'humphrey is',\n",
        " 'jack’s' : 'jack is',\n",
        " 'arthur’s': 'arthur is',\n",
        " 'lana’s': 'lana is',\n",
        " 'sarah’s': 'sarah is',\n",
        " 'garcia’s' : 'garcia is',\n",
        " 'ivan’s' : 'ivan is',\n",
        " 'zossimov’s' : 'zossimov is',\n",
        " 'totski’s' : 'totski is',\n",
        " 'miusov’s' : 'miusov is',\n",
        " 'rodya’s' : 'rodya is',\n",
        " 'odin’s' : 'odin is',\n",
        " 'maman’s' : 'maman is',\n",
        " 'thee’s' : 'thee is',\n",
        " 'ye’s' : 'ye is',\n",
        " 'richard’s' : 'richard is',\n",
        " 'silas’s' : 'silas is',\n",
        " 'von’s': 'von is',\n",
        " 'lanyon’s' : 'lanyon is',\n",
        " 'jack’s' : 'jack is',\n",
        " 'gunn’s' : 'gumn is',\n",
        " 'nay’s' : 'nay is',  \n",
        " 'rankeillor’s': 'rankeillor is',      \n",
        " 'odin\\'s' : 'odin is',\n",
        " 'joe\\'s' : 'joe is',\n",
        " 'dora\\'s' : 'dora is',\n",
        " 'wickfield\\'s' : 'wickfield is',\n",
        " 'tellson\\'s' : 'tellson is',\n",
        " 'omer\\'s' :  'omer is',\n",
        " 'cruncher\\'s' : 'crucher is', \n",
        " 'pip\\'s' : 'pip is',\n",
        " 'creakle\\'s ': 'creakle is',\n",
        " 'jorkins\\'s ' : 'jorkins is',\n",
        " 'jane\\'s' : 'jane is',\n",
        " 'elliot\\'s' : 'elliot is',\n",
        " 'anne\\'s' : 'anne is',\n",
        " 'tilney\\'s' : 'tilney is',\n",
        " 'lizzy\\'s' : 'lizzy is',\n",
        " 'smith\\'s' : 'smith is',\n",
        " 'walter\\'s' : 'walter is',\n",
        " 'musgrove\\'s' : 'musgrove is',\n",
        " 'lucy\\'s' : 'lucy is',\n",
        " 'nigel\\'s' : 'nigel is',\n",
        " 'nay\\'s' : 'nay is',\n",
        " 'chodinger\\'s' : 'chodinger is',\n",
        " 'humphrey\\'s' : 'humphrey is',\n",
        " 'jack\\'s' : 'jack is',\n",
        " 'arthur\\'s': 'arthur is',\n",
        " 'lana\\'s': 'lana is',\n",
        " 'sarah\\'s': 'sarah is',\n",
        " 'garcia\\'s' : 'garcia is',\n",
        " 'ivan\\'s' : 'ivan is',\n",
        " 'zossimov\\'s' : 'zossimov is',\n",
        " 'totski\\'s' : 'totski is',\n",
        " 'miusov\\'s' : 'miusov is',\n",
        " 'rodya\\'s' : 'rodya is',\n",
        " 'odin\\'s' : 'odin is',\n",
        " 'maman\\'s' : 'maman is',\n",
        " 'thee\\'s' : 'thee is',\n",
        " 'ye\\'s' : 'ye is',\n",
        " 'richard\\'s' : 'richard is',\n",
        " 'silas\\'s' : 'silas is',\n",
        " 'von\\'s': 'von is',\n",
        " 'lanyon\\'s' : 'lanyon is',\n",
        " 'jack\\'s' : 'jack is',\n",
        " 'gunn\\'s' : 'gumn is',\n",
        " 'nay\\'s' : 'nay is',  \n",
        " 'rankeillor\\'s': 'rankeillor is',          \n",
        " '\\'odin\\'s' : 'odin is',\n",
        " '\\'joe\\'s' : 'joe is',\n",
        " '\\'dora\\'s' : 'dora is',\n",
        " '\\'wickfield\\'s' : 'wickfield is',\n",
        " '\\'tellson\\'s' : 'tellson is',\n",
        " '\\'omer\\'s' :  'omer is',\n",
        " '\\'cruncher\\'s' : 'crucher is', \n",
        " '\\'pip\\'s' : 'pip is',\n",
        " '\\'creakle\\'s ': 'creakle is',\n",
        " '\\'jorkins\\'s ' : 'jorkins is',\n",
        " '\\'jane\\'s' : 'jane is',\n",
        " '\\'elliot\\'s' : 'elliot is',\n",
        " '\\'anne\\'s' : 'anne is',\n",
        " '\\'tilney\\'s' : 'tilney is',\n",
        " '\\'lizzy\\'s' : 'lizzy is',\n",
        " '\\'smith\\'s' : 'smith is',\n",
        " '\\'walter\\'s' : 'walter is',\n",
        " '\\'musgrove\\'s' : 'musgrove is',\n",
        " '\\'lucy\\'s' : 'lucy is',\n",
        " '\\'nigel\\'s' : 'nigel is',\n",
        " '\\'nay\\'s' : 'nay is',\n",
        " '\\'chodinger\\'s' : 'chodinger is',\n",
        " '\\'humphrey\\'s' : 'humphrey is',\n",
        " '\\'jack\\'s' : 'jack is',\n",
        " '\\'arthur\\'s': 'arthur is',\n",
        " '\\'lana\\'s': 'lana is',\n",
        " '\\'sarah\\'s': 'sarah is',\n",
        " '\\'garcia\\'s' : 'garcia is',\n",
        " '\\'ivan\\'s' : 'ivan is',\n",
        " '\\'zossimov\\'s' : 'zossimov is',\n",
        " '\\'totski\\'s' : 'totski is',\n",
        " '\\'miusov\\'s' : 'miusov is',\n",
        " '\\'rodya\\'s' : 'rodya is',\n",
        " '\\'odin\\'s' : 'odin is',\n",
        " '\\'maman\\'s' : 'maman is',\n",
        " '\\'thee\\'s' : 'thee is',\n",
        " '\\'ye\\'s' : 'ye is',\n",
        " '\\'richard\\'s' : 'richard is',\n",
        " '\\'silas\\'s' : 'silas is',\n",
        " '\\'von\\'s': 'von is',\n",
        " '\\'lanyon\\'s' : 'lanyon is',\n",
        " '\\'jack\\'s' : 'jack is',\n",
        " '\\'gunn\\'s' : 'gumn is',\n",
        " '\\'nay\\'s' : 'nay is',  \n",
        " '\\'rankeillor\\'s': 'rankeillor is',        \n",
        " 'he’s': 'he is',\n",
        " 'how’d': 'how did',\n",
        " 'how’re': 'how are',\n",
        " 'how’d’y': 'how do you',\n",
        " 'how’ll': 'how will',\n",
        " 'how’s': 'how is',\n",
        " 'I’d': 'I would',\n",
        " 'I’d’ve': 'I would have',\n",
        " 'I’ll': 'I will',\n",
        " 'I’ll’ve': 'I will have',\n",
        " 'I’m': 'I am',\n",
        " 'I’ve': 'I have',\n",
        " 'isn’t': 'is not',\n",
        " 'it’d': 'it would',\n",
        " 'it’d’ve': 'it would have',\n",
        " 'it’ll': 'it will',\n",
        " 'it’ll’ve': 'it will have',\n",
        " 'it’s': 'it is',\n",
        " 'let’s': 'let us',          \n",
        " 'ma’am': 'madam',\n",
        " 'mayn’t': 'may not',\n",
        " 'might’ve': 'might have',\n",
        " 'mightn’t': 'might not',\n",
        " 'mightn’t’ve': 'might not have',\n",
        " 'must’ve': 'must have',\n",
        " 'mustn’t': 'must not',\n",
        " 'mustn’t’ve': 'must not have',\n",
        " 'needn’t': 'need not',\n",
        " 'needn’t’ve': 'need not have',\n",
        " 'o’clock': 'of the clock',\n",
        " 'oughtn’t': 'ought not',\n",
        " 'oughtn’t’ve': 'ought not have',\n",
        " 'shan’t': 'shall not',\n",
        " 'sha’n’t': 'shall not',\n",
        " 'shan’t’ve': 'shall not have',\n",
        " '\\'she’d': 'she would',\n",
        " '\\'she’d’ve': 'she would have',\n",
        " '\\'she’ll': 'she will',\n",
        " '\\'she’ll’ve': 'she will have',\n",
        " '\\'she’s': 'she is',\n",
        " '\\'should’ve': 'should have',\n",
        " '\\'shouldn’t': 'should not',\n",
        " '\\'shouldn’t’ve': 'should not have',\n",
        " '\\'so’ve': 'so have',\n",
        " '\\'so’s': 'so is',\n",
        " '\\'that’d': 'that would',\n",
        " '\\'that’d’ve': 'that would have',\n",
        " '\\'that’s': 'that is',\n",
        " '\\'there’d': 'there would',\n",
        " '\\'there’d’ve': 'there would have',\n",
        " '\\'there’s': 'there is',\n",
        " '\\'they’d': 'they would',\n",
        " '\\'they’d’ve': 'they would have',\n",
        " '\\'they’ll': 'they will',\n",
        " '\\'they’ll’ve': 'they will have',\n",
        " '\\'they’re': 'they are',\n",
        " '\\'they’ve': 'they have',\n",
        " 'she’d': 'she would',\n",
        " 'she’d’ve': 'she would have',\n",
        " 'she’ll': 'she will',\n",
        " 'she’ll’ve': 'she will have',\n",
        " 'she’s': 'she is',\n",
        " 'should’ve': 'should have',\n",
        " 'shouldn’t': 'should not',\n",
        " 'shouldn’t’ve': 'should not have',\n",
        " 'so’ve': 'so have',\n",
        " 'so’s': 'so is',\n",
        " 'that’d': 'that would',\n",
        " 'that’d’ve': 'that would have',\n",
        " 'that’s': 'that is',\n",
        " 'there’d': 'there would',\n",
        " 'there’d’ve': 'there would have',\n",
        " 'there’s': 'there is',\n",
        " 'they’d': 'they would',\n",
        " 'they’d’ve': 'they would have',\n",
        " 'they’ll': 'they will',\n",
        " 'they’ll’ve': 'they will have',\n",
        " 'they’re': 'they are',\n",
        " 'they’ve': 'they have',      \n",
        " 'to’ve': 'to have',\n",
        " 'wasn’t': 'was not',\n",
        " '\\'we’d': 'we would',\n",
        " '\\'we’d’ve': 'we would have',\n",
        " '\\'we’ll': 'we will',\n",
        " '\\'we’ll’ve': 'we will have',\n",
        " '\\'we’re': 'we are',\n",
        " '\\'we’ve': 'we have',\n",
        " 'we’d': 'we would',\n",
        " 'we’d’ve': 'we would have',\n",
        " 'we’ll': 'we will',\n",
        " 'we’ll’ve': 'we will have',\n",
        " 'we’re': 'we are',\n",
        " 'we’ve': 'we have',          \n",
        " 'weren’t': 'were not',\n",
        " '\\'what’ll': 'what will',\n",
        " '\\'what’ll’ve': 'what will have',\n",
        " '\\'what’re': 'what are',\n",
        " '\\'what’s': 'what is',\n",
        " '\\'what’ve': 'what have',\n",
        " '\\'when’s': 'when is',\n",
        " '\\'when’ve': 'when have',\n",
        " '\\'where’d': 'where did',\n",
        " '\\'where’s': 'where is',\n",
        " '\\'where’ve': 'where have',\n",
        " '\\'who’ll': 'who will',\n",
        " '\\'who’ll’ve': 'who will have',\n",
        " '\\'who’s': 'who is',\n",
        " '\\'who’ve': 'who have',\n",
        " '\\'why’s': 'why is',\n",
        " '\\'why’ve': 'why have',\n",
        " '\\'will’ve': 'will have',\n",
        " '\\'won’t': 'will not',\n",
        " '\\'won’t’ve': 'will not have',\n",
        " '\\'would’ve': 'would have',\n",
        " '\\'wouldn’t': 'would not',\n",
        " '\\'wouldn’t’ve': 'would not have',\n",
        " 'what’ll': 'what will',\n",
        " 'what’ll’ve': 'what will have',\n",
        " 'what’re': 'what are',\n",
        " 'what’s': 'what is',\n",
        " 'what’ve': 'what have',\n",
        " 'when’s': 'when is',\n",
        " 'when’ve': 'when have',\n",
        " 'where’d': 'where did',\n",
        " 'where’s': 'where is',\n",
        " 'where’ve': 'where have',\n",
        " 'who’ll': 'who will',\n",
        " 'who’ll’ve': 'who will have',\n",
        " 'who’s': 'who is',\n",
        " 'who’ve': 'who have',\n",
        " 'why’s': 'why is',\n",
        " 'why’ve': 'why have',\n",
        " 'will’ve': 'will have',\n",
        " 'won’t': 'will not',\n",
        " 'won’t’ve': 'will not have',\n",
        " 'would’ve': 'would have',\n",
        " 'wouldn’t': 'would not',\n",
        " 'wouldn’t’ve': 'would not have',   \n",
        " 'y’all': 'you all',\n",
        " 'y’all’d': 'you all would',\n",
        " 'y’all’d’ve': 'you all would have',\n",
        " 'y’all’re': 'you all are',\n",
        " 'y’all’ve': 'you all have',\n",
        " '\\'you’d': 'you would',\n",
        " '\\'you’d’ve': 'you would have',\n",
        " '\\'you’ll': 'you will',\n",
        " '\\'you’ll’ve': 'you shall have',\n",
        " '\\'you’re': 'you are',\n",
        " '\\'you’ve': 'you have', \n",
        " 'you’d': 'you would',\n",
        " 'you’d’ve': 'you would have',\n",
        " 'you’ll': 'you will',\n",
        " 'you’ll’ve': 'you shall have',\n",
        " 'you’re': 'you are',\n",
        " 'you’ve': 'you have'\n",
        "}"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyJjxRBvlmYz"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "def clean_contraction(text):\n",
        "    words = text_to_word_sequence(text)\n",
        "    words=[cont_dict[word] if word in cont_dict else word for word in words]\n",
        "    clean_sent=\" \".join(words)\n",
        "    \n",
        "    return clean_sent\n",
        "\n",
        "train['text'] = train['text'].str.lower().apply(clean_contraction)\n",
        "test['text'] = test['text'].str.lower().apply(clean_contraction)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBbMM9Z0l1yO"
      },
      "source": [
        "train['text']=train['text'].str.replace('\\'s', '')\n",
        "train['text']=train['text'].str.replace('’s', '')\n",
        "train['text']=train['text'].str.replace(\"\\'\", '')\n",
        "train['text']=train['text'].str.replace(\"’\", '')\n",
        "\n",
        "test['text']=test['text'].str.replace(\"’s\",'')\n",
        "test['text']=test['text'].str.replace(\"\\'s\",'')\n",
        "test['text']=test['text'].str.replace(\"\\'\", '')\n",
        "test['text']=test['text'].str.replace(\"’\", '')"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zOl9NoFl3E8"
      },
      "source": [
        "train['text']=train['text'].str.replace('á', '')\n",
        "train['text']=train['text'].str.replace('ä', '')\n",
        "train['text']=train['text'].str.replace('é', '')\n",
        "train['text']=train['text'].str.replace('í', '')\n",
        "train['text']=train['text'].str.replace('ó', '')\n",
        "train['text']=train['text'].str.replace('ú', '')\n",
        "train['text']=train['text'].str.replace('ý', '')\n",
        "train['text']=train['text'].str.replace('ü', ' Umlaut ')\n",
        "\n",
        "test['text']=test['text'].str.replace('ä', '')\n",
        "test['text']=test['text'].str.replace('á', '')\n",
        "test['text']=test['text'].str.replace('é', '')\n",
        "test['text']=test['text'].str.replace('í', '')\n",
        "test['text']=test['text'].str.replace('ó', '')\n",
        "test['text']=test['text'].str.replace('ú', '')\n",
        "test['text']=test['text'].str.replace('ý', '')\n",
        "test['text']=test['text'].str.replace('ü', ' Umlaut ')"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPQ_Hm9zl6Sx",
        "outputId": "9a6a4777-d5b0-47e9-f4b2-679ee1da8a7c"
      },
      "source": [
        "\n",
        "X_train = train['text'].values\n",
        "X_test = test['text'].values\n",
        "y = train['author'].values\n",
        "print(X_train.shape, X_test.shape, y.shape)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(54879,) (19617,) (54879,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqaiYqE4l__2"
      },
      "source": [
        "\n",
        "vocab_size = 20000\n",
        "embedding_dim = 128\n",
        "max_length = 260\n",
        "padding_type='post'"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSOBSLQkmCVb"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCQNCTRomDua"
      },
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XocVc05EmF0W",
        "outputId": "2e008c7c-69df-4825-ceb0-f651ec25c684"
      },
      "source": [
        "trn = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n",
        "tst = pad_sequences(test_sequences, padding=padding_type, maxlen=max_length)\n",
        "print(trn.shape, tst.shape)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(54879, 260) (19617, 260)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU9Em_qomHKs"
      },
      "source": [
        "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9Ys9xK7mIq7"
      },
      "source": [
        "def get_model():\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        Bidirectional(LSTM(128, return_sequences=True)),\n",
        "        Bidirectional(LSTM(128)),\n",
        "        Dense(n_class, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=.01))\n",
        "    return model"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5dXEkR-mKC9",
        "outputId": "639e26ba-2983-4b9a-9207-f85d350f1089"
      },
      "source": [
        "p_val = np.zeros((trn.shape[0], n_class))\n",
        "p_tst = np.zeros((tst.shape[0], n_class))\n",
        "for i, (i_trn, i_val) in enumerate(cv.split(trn, y), 1):\n",
        "    print(f'training model for CV #{i}')\n",
        "    clf = get_model()\n",
        "    \n",
        "    es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n",
        "                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
        "\n",
        "    clf.fit(trn[i_trn], \n",
        "            to_categorical(y[i_trn]),\n",
        "            validation_data=(trn[i_val], to_categorical(y[i_val])),\n",
        "            epochs=10,\n",
        "            batch_size=256,\n",
        "            callbacks=[es])\n",
        "    p_val[i_val, :] = clf.predict(trn[i_val])\n",
        "    p_tst += clf.predict(tst) / n_fold"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training model for CV #1\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 31s 183ms/step - loss: 1.0505 - val_loss: 0.7154\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 30s 177ms/step - loss: 0.5449 - val_loss: 0.6227\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 31s 179ms/step - loss: 0.3712 - val_loss: 0.6822\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 31s 180ms/step - loss: 0.2773 - val_loss: 0.7239\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - ETA: 0s - loss: 0.2207Restoring model weights from the end of the best epoch.\n",
            "172/172 [==============================] - 31s 181ms/step - loss: 0.2207 - val_loss: 0.8055\n",
            "Epoch 00005: early stopping\n",
            "training model for CV #2\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 32s 186ms/step - loss: 1.0582 - val_loss: 0.7520\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 31s 180ms/step - loss: 0.5548 - val_loss: 0.6345\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 31s 180ms/step - loss: 0.3843 - val_loss: 0.6781\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 31s 180ms/step - loss: 0.2973 - val_loss: 0.7150\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - ETA: 0s - loss: 0.2385Restoring model weights from the end of the best epoch.\n",
            "172/172 [==============================] - 31s 181ms/step - loss: 0.2385 - val_loss: 0.7614\n",
            "Epoch 00005: early stopping\n",
            "training model for CV #3\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 32s 188ms/step - loss: 1.0726 - val_loss: 0.7270\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 31s 182ms/step - loss: 0.5519 - val_loss: 0.6194\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 31s 181ms/step - loss: 0.3822 - val_loss: 0.6933\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 31s 181ms/step - loss: 0.2887 - val_loss: 0.7364\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - ETA: 0s - loss: 0.2299Restoring model weights from the end of the best epoch.\n",
            "172/172 [==============================] - 31s 181ms/step - loss: 0.2299 - val_loss: 0.7801\n",
            "Epoch 00005: early stopping\n",
            "training model for CV #4\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 32s 187ms/step - loss: 1.0344 - val_loss: 0.7408\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 31s 181ms/step - loss: 0.5425 - val_loss: 0.6325\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 31s 181ms/step - loss: 0.3761 - val_loss: 0.6766\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 31s 181ms/step - loss: 0.2795 - val_loss: 0.6988\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - ETA: 0s - loss: 0.2194Restoring model weights from the end of the best epoch.\n",
            "172/172 [==============================] - 31s 181ms/step - loss: 0.2194 - val_loss: 0.8669\n",
            "Epoch 00005: early stopping\n",
            "training model for CV #5\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 32s 188ms/step - loss: 1.0379 - val_loss: 0.7924\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 31s 181ms/step - loss: 0.5714 - val_loss: 0.6389\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 31s 181ms/step - loss: 0.3917 - val_loss: 0.6507\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 31s 181ms/step - loss: 0.2950 - val_loss: 0.7055\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - ETA: 0s - loss: 0.2366Restoring model weights from the end of the best epoch.\n",
            "172/172 [==============================] - 31s 182ms/step - loss: 0.2366 - val_loss: 0.7799\n",
            "Epoch 00005: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk4cSNJnmMHE",
        "outputId": "c9e512d0-2290-418a-b1f5-42e8937d92de"
      },
      "source": [
        "print(f'Accuracy (CV): {accuracy_score(y, np.argmax(p_val, axis=1)) * 100:8.4f}%')\n",
        "print(f'Log Loss (CV): {log_loss(pd.get_dummies(y), p_val):8.4f}')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy (CV):  77.3301%\n",
            "Log Loss (CV):   0.6296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flM8_PzHmPuY"
      },
      "source": [
        "np.savetxt(p_val_file, p_val, fmt='%.6f', delimiter=',')\n",
        "np.savetxt(p_tst_file, p_tst, fmt='%.6f', delimiter=',')\n"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "8wyR9Au2mUZd",
        "outputId": "f630e8fa-0e7c-4ecb-9470-6f4b74f959cd"
      },
      "source": [
        "sub = pd.read_csv(sample_file, index_col=0)\n",
        "print(sub.shape)\n",
        "sub.head()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19617, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0  1  2  3  4\n",
              "index               \n",
              "0      0  0  0  0  0\n",
              "1      0  0  0  0  0\n",
              "2      0  0  0  0  0\n",
              "3      0  0  0  0  0\n",
              "4      0  0  0  0  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "LvYmw0scmU5Y",
        "outputId": "327ce9ec-b49f-46fe-b425-75d9324a477d"
      },
      "source": [
        "\n",
        "sub[sub.columns] = p_tst\n",
        "sub.head()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0509</td>\n",
              "      <td>0.6071</td>\n",
              "      <td>0.2614</td>\n",
              "      <td>0.0674</td>\n",
              "      <td>0.0131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.3381</td>\n",
              "      <td>0.3665</td>\n",
              "      <td>0.0283</td>\n",
              "      <td>0.0688</td>\n",
              "      <td>0.1983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.9545</td>\n",
              "      <td>0.0223</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0058</td>\n",
              "      <td>0.0079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0208</td>\n",
              "      <td>0.0123</td>\n",
              "      <td>0.9373</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.6811</td>\n",
              "      <td>0.0550</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.0992</td>\n",
              "      <td>0.0446</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0       1       2       3       4\n",
              "index                                        \n",
              "0      0.0509  0.6071  0.2614  0.0674  0.0131\n",
              "1      0.3381  0.3665  0.0283  0.0688  0.1983\n",
              "2      0.9545  0.0223  0.0095  0.0058  0.0079\n",
              "3      0.0208  0.0123  0.9373  0.0084  0.0212\n",
              "4      0.6811  0.0550  0.1200  0.0992  0.0446"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH04Rop5mWQs"
      },
      "source": [
        "\n",
        "sub.to_csv(sub_file)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtqhodxKmX8b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
