{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T06:24:21.290893Z",
     "start_time": "2020-10-06T06:24:20.568289Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T06:24:25.400482Z",
     "start_time": "2020-10-06T06:24:21.800804Z"
    }
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import numpy as np\n",
    "import optuna.integration.lightgbm as lgb\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T06:24:28.423970Z",
     "start_time": "2020-10-06T06:24:27.622469Z"
    }
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (16, 8)\n",
    "plt.style.use('fivethirtyeight')\n",
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T06:24:29.840532Z",
     "start_time": "2020-10-06T06:24:29.043955Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = Path('../input')\n",
    "feature_dir = Path('../feature')\n",
    "val_dir = Path('../txt')\n",
    "tst_dir = Path('../txt')\n",
    "sub_dir = Path('../submission')\n",
    "\n",
    "trn_file = data_dir / 'train.csv'\n",
    "tst_file = data_dir / 'test.csv'\n",
    "sample_file = sub_dir / 'sample_submission.csv'\n",
    "\n",
    "target_col = 'class'\n",
    "n_fold = 5\n",
    "n_class = 3\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T06:24:31.730467Z",
     "start_time": "2020-10-06T06:24:30.597452Z"
    }
   },
   "outputs": [],
   "source": [
    "algo_name = 'lgb_optuna'\n",
    "feature_name = 'feature'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "feature_file = feature_dir / f'{feature_name}.csv'\n",
    "p_val_file = val_dir / f'{model_name}.val.csv'\n",
    "p_tst_file = tst_dir / f'{model_name}.tst.csv'\n",
    "sub_file = sub_dir / f'{model_name}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T06:24:37.453219Z",
     "start_time": "2020-10-06T06:24:32.352138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>z</th>\n",
       "      <th>redshift</th>\n",
       "      <th>dered_u</th>\n",
       "      <th>dered_g</th>\n",
       "      <th>dered_r</th>\n",
       "      <th>dered_i</th>\n",
       "      <th>dered_z</th>\n",
       "      <th>nObserve</th>\n",
       "      <th>airmass_u</th>\n",
       "      <th>class</th>\n",
       "      <th>g_r</th>\n",
       "      <th>u_g</th>\n",
       "      <th>r_i</th>\n",
       "      <th>i_z</th>\n",
       "      <th>u_r</th>\n",
       "      <th>d_dered_u</th>\n",
       "      <th>d_dered_g</th>\n",
       "      <th>d_dered_r</th>\n",
       "      <th>d_dered_i</th>\n",
       "      <th>d_dered_z</th>\n",
       "      <th>d_dered_ug</th>\n",
       "      <th>d_dered_gr</th>\n",
       "      <th>d_dered_iz</th>\n",
       "      <th>d_dered_ur</th>\n",
       "      <th>d_dered_ri</th>\n",
       "      <th>d_obs_det</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.9396</td>\n",
       "      <td>-8.1086e-05</td>\n",
       "      <td>23.1243</td>\n",
       "      <td>20.2578</td>\n",
       "      <td>18.9551</td>\n",
       "      <td>17.6321</td>\n",
       "      <td>16.9089</td>\n",
       "      <td>1.3723</td>\n",
       "      <td>1.1898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3273</td>\n",
       "      <td>2.9272</td>\n",
       "      <td>1.3371</td>\n",
       "      <td>0.7328</td>\n",
       "      <td>4.2545</td>\n",
       "      <td>-0.1397</td>\n",
       "      <td>-0.0790</td>\n",
       "      <td>-0.0544</td>\n",
       "      <td>-0.0403</td>\n",
       "      <td>-0.0307</td>\n",
       "      <td>2.8665</td>\n",
       "      <td>1.3027</td>\n",
       "      <td>0.7232</td>\n",
       "      <td>4.1691</td>\n",
       "      <td>1.3230</td>\n",
       "      <td>2.1456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.1689</td>\n",
       "      <td>4.5061e-03</td>\n",
       "      <td>14.9664</td>\n",
       "      <td>14.0045</td>\n",
       "      <td>13.4114</td>\n",
       "      <td>13.2363</td>\n",
       "      <td>13.1347</td>\n",
       "      <td>0.5266</td>\n",
       "      <td>1.2533</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6096</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>1.5997</td>\n",
       "      <td>-0.0857</td>\n",
       "      <td>-0.0574</td>\n",
       "      <td>-0.0410</td>\n",
       "      <td>-0.0322</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>0.9619</td>\n",
       "      <td>0.5931</td>\n",
       "      <td>0.1016</td>\n",
       "      <td>1.5550</td>\n",
       "      <td>0.1751</td>\n",
       "      <td>1.3163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.3500</td>\n",
       "      <td>4.7198e-04</td>\n",
       "      <td>16.6076</td>\n",
       "      <td>15.6866</td>\n",
       "      <td>15.4400</td>\n",
       "      <td>15.3217</td>\n",
       "      <td>15.2961</td>\n",
       "      <td>0.7413</td>\n",
       "      <td>1.0225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2891</td>\n",
       "      <td>0.9610</td>\n",
       "      <td>0.1428</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>1.2501</td>\n",
       "      <td>-0.1787</td>\n",
       "      <td>-0.1388</td>\n",
       "      <td>-0.0963</td>\n",
       "      <td>-0.0718</td>\n",
       "      <td>-0.0540</td>\n",
       "      <td>0.9211</td>\n",
       "      <td>0.2466</td>\n",
       "      <td>0.0257</td>\n",
       "      <td>1.1676</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>1.4821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.6346</td>\n",
       "      <td>5.8143e-06</td>\n",
       "      <td>25.3536</td>\n",
       "      <td>20.9947</td>\n",
       "      <td>20.0873</td>\n",
       "      <td>19.7947</td>\n",
       "      <td>19.5552</td>\n",
       "      <td>0.9591</td>\n",
       "      <td>1.2054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9676</td>\n",
       "      <td>4.4719</td>\n",
       "      <td>0.3262</td>\n",
       "      <td>0.2603</td>\n",
       "      <td>5.4395</td>\n",
       "      <td>-0.3070</td>\n",
       "      <td>-0.1941</td>\n",
       "      <td>-0.1339</td>\n",
       "      <td>-0.1003</td>\n",
       "      <td>-0.0795</td>\n",
       "      <td>4.3590</td>\n",
       "      <td>0.9074</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>5.2664</td>\n",
       "      <td>0.2926</td>\n",
       "      <td>1.4454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.9826</td>\n",
       "      <td>-3.3247e-05</td>\n",
       "      <td>23.7714</td>\n",
       "      <td>20.4338</td>\n",
       "      <td>18.8630</td>\n",
       "      <td>18.1903</td>\n",
       "      <td>17.8759</td>\n",
       "      <td>1.2917</td>\n",
       "      <td>1.1939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6568</td>\n",
       "      <td>3.7543</td>\n",
       "      <td>0.7182</td>\n",
       "      <td>0.3415</td>\n",
       "      <td>5.4111</td>\n",
       "      <td>-0.6820</td>\n",
       "      <td>-0.2653</td>\n",
       "      <td>-0.1794</td>\n",
       "      <td>-0.1339</td>\n",
       "      <td>-0.1067</td>\n",
       "      <td>3.3376</td>\n",
       "      <td>1.5709</td>\n",
       "      <td>0.3144</td>\n",
       "      <td>4.9084</td>\n",
       "      <td>0.6727</td>\n",
       "      <td>1.9857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          z    redshift  dered_u  dered_g  dered_r  dered_i  dered_z  \\\n",
       "id                                                                     \n",
       "0   16.9396 -8.1086e-05  23.1243  20.2578  18.9551  17.6321  16.9089   \n",
       "1   13.1689  4.5061e-03  14.9664  14.0045  13.4114  13.2363  13.1347   \n",
       "2   15.3500  4.7198e-04  16.6076  15.6866  15.4400  15.3217  15.2961   \n",
       "3   19.6346  5.8143e-06  25.3536  20.9947  20.0873  19.7947  19.5552   \n",
       "4   17.9826 -3.3247e-05  23.7714  20.4338  18.8630  18.1903  17.8759   \n",
       "\n",
       "    nObserve  airmass_u  class     g_r     u_g     r_i     i_z     u_r  \\\n",
       "id                                                                       \n",
       "0     1.3723     1.1898    0.0  1.3273  2.9272  1.3371  0.7328  4.2545   \n",
       "1     0.5266     1.2533    1.0  0.6096  0.9902  0.1840  0.0995  1.5997   \n",
       "2     0.7413     1.0225    0.0  0.2891  0.9610  0.1428  0.0435  1.2501   \n",
       "3     0.9591     1.2054    0.0  0.9676  4.4719  0.3262  0.2603  5.4395   \n",
       "4     1.2917     1.1939    0.0  1.6568  3.7543  0.7182  0.3415  5.4111   \n",
       "\n",
       "    d_dered_u  d_dered_g  d_dered_r  d_dered_i  d_dered_z  d_dered_ug  \\\n",
       "id                                                                      \n",
       "0     -0.1397    -0.0790    -0.0544    -0.0403    -0.0307      2.8665   \n",
       "1     -0.0857    -0.0574    -0.0410    -0.0322    -0.0343      0.9619   \n",
       "2     -0.1787    -0.1388    -0.0963    -0.0718    -0.0540      0.9211   \n",
       "3     -0.3070    -0.1941    -0.1339    -0.1003    -0.0795      4.3590   \n",
       "4     -0.6820    -0.2653    -0.1794    -0.1339    -0.1067      3.3376   \n",
       "\n",
       "    d_dered_gr  d_dered_iz  d_dered_ur  d_dered_ri  d_obs_det  \n",
       "id                                                             \n",
       "0       1.3027      0.7232      4.1691      1.3230     2.1456  \n",
       "1       0.5931      0.1016      1.5550      0.1751     1.3163  \n",
       "2       0.2466      0.0257      1.1676      0.1183     1.4821  \n",
       "3       0.9074      0.2395      5.2664      0.2926     1.4454  \n",
       "4       1.5709      0.3144      4.9084      0.6727     1.9857  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(feature_file, index_col=0)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T06:24:39.807367Z",
     "start_time": "2020-10-06T06:24:38.897888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320000,) (320000, 25) (80000, 25)\n"
     ]
    }
   ],
   "source": [
    "y = df[target_col].values[:320000]\n",
    "df.drop(target_col, axis=1, inplace=True)\n",
    "trn = df.iloc[:320000].values\n",
    "tst = df.iloc[320000:].values\n",
    "feature_name = df.columns.tolist()\n",
    "print(y.shape, trn.shape, tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T06:24:41.553990Z",
     "start_time": "2020-10-06T06:24:40.521531Z"
    }
   },
   "outputs": [],
   "source": [
    "X_trn, X_val, y_trn, y_val = train_test_split(trn, y, test_size=.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T06:24:43.526109Z",
     "start_time": "2020-10-06T06:24:42.421741Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"metric\": \"multi_logloss\",\n",
    "    \"num_class\": 3,\n",
    "    \"n_estimators\": 1000,\n",
    "    \"subsample_freq\": 1,\n",
    "    \"lambda_l1\": 0.,\n",
    "    \"lambda_l2\": 0.,\n",
    "    \"random_state\": seed,\n",
    "    \"n_jobs\": -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T07:05:39.829841Z",
     "start_time": "2020-10-06T06:24:44.776867Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-10-06 15:24:45,747] A new study created in memory with name: no-name-9b105626-6230-4c48-b7ef-8a1cc429ec6c\n",
      "feature_fraction, val_score: inf:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085804 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[75]\ttraining's multi_logloss: 0.148563\tvalid_1's multi_logloss: 0.161707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.161707:  14%|#4        | 1/7 [00:20<02:04, 20.82s/it][I 2020-10-06 15:25:06,583] Trial 0 finished with value: 0.1617067517283609 and parameters: {'feature_fraction': 1.0}. Best is trial 0 with value: 0.1617067517283609.\n",
      "feature_fraction, val_score: 0.161707:  14%|#4        | 1/7 [00:20<02:04, 20.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.143883\tvalid_1's multi_logloss: 0.160292\n",
      "[200]\ttraining's multi_logloss: 0.131677\tvalid_1's multi_logloss: 0.158459\n",
      "Early stopping, best iteration is:\n",
      "[259]\ttraining's multi_logloss: 0.125949\tvalid_1's multi_logloss: 0.15809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.158090:  29%|##8       | 2/7 [01:14<02:33, 30.71s/it][I 2020-10-06 15:26:00,378] Trial 1 finished with value: 0.15809041897531523 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 1 with value: 0.15809041897531523.\n",
      "feature_fraction, val_score: 0.158090:  29%|##8       | 2/7 [01:14<02:33, 30.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152078 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.145734\tvalid_1's multi_logloss: 0.160264\n",
      "[200]\ttraining's multi_logloss: 0.132933\tvalid_1's multi_logloss: 0.157908\n",
      "Early stopping, best iteration is:\n",
      "[235]\ttraining's multi_logloss: 0.129646\tvalid_1's multi_logloss: 0.157713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.157713:  43%|####2     | 3/7 [02:18<02:42, 40.61s/it][I 2020-10-06 15:27:04,078] Trial 2 finished with value: 0.15771333644759597 and parameters: {'feature_fraction': 0.6}. Best is trial 2 with value: 0.15771333644759597.\n",
      "feature_fraction, val_score: 0.157713:  43%|####2     | 3/7 [02:18<02:42, 40.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151584 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.14464\tvalid_1's multi_logloss: 0.160185\n",
      "[200]\ttraining's multi_logloss: 0.132013\tvalid_1's multi_logloss: 0.158034\n",
      "Early stopping, best iteration is:\n",
      "[233]\ttraining's multi_logloss: 0.128836\tvalid_1's multi_logloss: 0.157819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.157713:  57%|#####7    | 4/7 [03:04<02:07, 42.34s/it][I 2020-10-06 15:27:50,465] Trial 3 finished with value: 0.15781911911106056 and parameters: {'feature_fraction': 0.7}. Best is trial 2 with value: 0.15771333644759597.\n",
      "feature_fraction, val_score: 0.157713:  57%|#####7    | 4/7 [03:04<02:07, 42.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.146185\tvalid_1's multi_logloss: 0.160497\n",
      "Early stopping, best iteration is:\n",
      "[107]\ttraining's multi_logloss: 0.144981\tvalid_1's multi_logloss: 0.160335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.157713:  71%|#######1  | 5/7 [03:26<01:12, 36.07s/it][I 2020-10-06 15:28:11,905] Trial 4 finished with value: 0.16033490595652058 and parameters: {'feature_fraction': 0.5}. Best is trial 2 with value: 0.15771333644759597.\n",
      "feature_fraction, val_score: 0.157713:  71%|#######1  | 5/7 [03:26<01:12, 36.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084727 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.148933\tvalid_1's multi_logloss: 0.162193\n",
      "Early stopping, best iteration is:\n",
      "[170]\ttraining's multi_logloss: 0.138531\tvalid_1's multi_logloss: 0.158955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.157713:  86%|########5 | 6/7 [03:58<00:34, 34.88s/it][I 2020-10-06 15:28:44,010] Trial 5 finished with value: 0.1589547262960009 and parameters: {'feature_fraction': 0.4}. Best is trial 2 with value: 0.15771333644759597.\n",
      "feature_fraction, val_score: 0.157713:  86%|########5 | 6/7 [03:58<00:34, 34.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125768 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[82]\ttraining's multi_logloss: 0.147683\tvalid_1's multi_logloss: 0.161202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.157713: 100%|##########| 7/7 [04:19<00:00, 30.86s/it][I 2020-10-06 15:29:05,485] Trial 6 finished with value: 0.16120201952254856 and parameters: {'feature_fraction': 0.8}. Best is trial 2 with value: 0.15771333644759597.\n",
      "feature_fraction, val_score: 0.157713: 100%|##########| 7/7 [04:19<00:00, 37.10s/it]\n",
      "num_leaves, val_score: 0.157713:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090930 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.101367\tvalid_1's multi_logloss: 0.156968\n",
      "Early stopping, best iteration is:\n",
      "[101]\ttraining's multi_logloss: 0.101049\tvalid_1's multi_logloss: 0.156935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156935:   5%|5         | 1/20 [00:31<10:01, 31.65s/it][I 2020-10-06 15:29:37,175] Trial 7 finished with value: 0.15693476778164636 and parameters: {'num_leaves': 185}. Best is trial 7 with value: 0.15693476778164636.\n",
      "num_leaves, val_score: 0.156935:   5%|5         | 1/20 [00:31<10:01, 31.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.13185\tvalid_1's multi_logloss: 0.158165\n",
      "Early stopping, best iteration is:\n",
      "[182]\ttraining's multi_logloss: 0.116482\tvalid_1's multi_logloss: 0.157134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156935:  10%|#         | 2/20 [01:12<10:18, 34.38s/it][I 2020-10-06 15:30:17,920] Trial 8 finished with value: 0.15713385702944435 and parameters: {'num_leaves': 66}. Best is trial 7 with value: 0.15693476778164636.\n",
      "num_leaves, val_score: 0.156935:  10%|#         | 2/20 [01:12<10:18, 34.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.127382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.091673\tvalid_1's multi_logloss: 0.156847\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttraining's multi_logloss: 0.0904789\tvalid_1's multi_logloss: 0.156804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156804:  15%|#5        | 3/20 [01:46<09:45, 34.42s/it][I 2020-10-06 15:30:52,425] Trial 9 finished with value: 0.15680387039922558 and parameters: {'num_leaves': 237}. Best is trial 9 with value: 0.15680387039922558.\n",
      "num_leaves, val_score: 0.156804:  15%|#5        | 3/20 [01:46<09:45, 34.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0882276\tvalid_1's multi_logloss: 0.157267\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's multi_logloss: 0.091585\tvalid_1's multi_logloss: 0.15717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156804:  20%|##        | 4/20 [02:22<09:17, 34.87s/it][I 2020-10-06 15:31:28,363] Trial 10 finished with value: 0.15716976968537988 and parameters: {'num_leaves': 256}. Best is trial 9 with value: 0.15680387039922558.\n",
      "num_leaves, val_score: 0.156804:  20%|##        | 4/20 [02:22<09:17, 34.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116726 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0945333\tvalid_1's multi_logloss: 0.157339\n",
      "Early stopping, best iteration is:\n",
      "[110]\ttraining's multi_logloss: 0.0910041\tvalid_1's multi_logloss: 0.157256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156804:  25%|##5       | 5/20 [03:12<09:51, 39.45s/it][I 2020-10-06 15:32:18,490] Trial 11 finished with value: 0.15725645179902506 and parameters: {'num_leaves': 221}. Best is trial 9 with value: 0.15680387039922558.\n",
      "num_leaves, val_score: 0.156804:  25%|##5       | 5/20 [03:12<09:51, 39.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.106282\tvalid_1's multi_logloss: 0.157343\n",
      "Early stopping, best iteration is:\n",
      "[105]\ttraining's multi_logloss: 0.104623\tvalid_1's multi_logloss: 0.157278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156804:  30%|###       | 6/20 [03:43<08:34, 36.73s/it][I 2020-10-06 15:32:48,858] Trial 12 finished with value: 0.15727805614773113 and parameters: {'num_leaves': 160}. Best is trial 9 with value: 0.15680387039922558.\n",
      "num_leaves, val_score: 0.156804:  30%|###       | 6/20 [03:43<08:34, 36.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089133 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.100383\tvalid_1's multi_logloss: 0.157001\n",
      "Early stopping, best iteration is:\n",
      "[101]\ttraining's multi_logloss: 0.100049\tvalid_1's multi_logloss: 0.156961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156804:  35%|###5      | 7/20 [04:17<07:47, 36.00s/it][I 2020-10-06 15:33:23,145] Trial 13 finished with value: 0.15696137220462367 and parameters: {'num_leaves': 189}. Best is trial 9 with value: 0.15680387039922558.\n",
      "num_leaves, val_score: 0.156804:  35%|###5      | 7/20 [04:17<07:47, 36.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0894568\tvalid_1's multi_logloss: 0.157168\n",
      "Early stopping, best iteration is:\n",
      "[101]\ttraining's multi_logloss: 0.0890386\tvalid_1's multi_logloss: 0.15715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156804:  40%|####      | 8/20 [04:51<07:03, 35.25s/it][I 2020-10-06 15:33:56,664] Trial 14 finished with value: 0.15714952013502292 and parameters: {'num_leaves': 250}. Best is trial 9 with value: 0.15680387039922558.\n",
      "num_leaves, val_score: 0.156804:  40%|####      | 8/20 [04:51<07:03, 35.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111808 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.117672\tvalid_1's multi_logloss: 0.157241\n",
      "Early stopping, best iteration is:\n",
      "[122]\ttraining's multi_logloss: 0.111457\tvalid_1's multi_logloss: 0.156993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156804:  45%|####5     | 9/20 [05:24<06:20, 34.60s/it][I 2020-10-06 15:34:29,743] Trial 15 finished with value: 0.15699263863422847 and parameters: {'num_leaves': 114}. Best is trial 9 with value: 0.15680387039922558.\n",
      "num_leaves, val_score: 0.156804:  45%|####5     | 9/20 [05:24<06:20, 34.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.133276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0995927\tvalid_1's multi_logloss: 0.157173\n",
      "Early stopping, best iteration is:\n",
      "[110]\ttraining's multi_logloss: 0.0960569\tvalid_1's multi_logloss: 0.157145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156804:  50%|#####     | 10/20 [05:56<05:39, 33.98s/it][I 2020-10-06 15:35:02,288] Trial 16 finished with value: 0.15714542201798395 and parameters: {'num_leaves': 193}. Best is trial 9 with value: 0.15680387039922558.\n",
      "num_leaves, val_score: 0.156804:  50%|#####     | 10/20 [05:56<05:39, 33.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.160921 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.112442\tvalid_1's multi_logloss: 0.157138\n",
      "Early stopping, best iteration is:\n",
      "[108]\ttraining's multi_logloss: 0.110066\tvalid_1's multi_logloss: 0.157016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156804:  55%|#####5    | 11/20 [06:26<04:53, 32.63s/it][I 2020-10-06 15:35:31,754] Trial 17 finished with value: 0.1570156840563474 and parameters: {'num_leaves': 133}. Best is trial 9 with value: 0.15680387039922558.\n",
      "num_leaves, val_score: 0.156804:  55%|#####5    | 11/20 [06:26<04:53, 32.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084036 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0952246\tvalid_1's multi_logloss: 0.157052\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttraining's multi_logloss: 0.0942304\tvalid_1's multi_logloss: 0.156988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156804:  60%|######    | 12/20 [06:58<04:19, 32.39s/it][I 2020-10-06 15:36:03,585] Trial 18 finished with value: 0.15698769324772413 and parameters: {'num_leaves': 215}. Best is trial 9 with value: 0.15680387039922558.\n",
      "num_leaves, val_score: 0.156804:  60%|######    | 12/20 [06:58<04:19, 32.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090286 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0881386\tvalid_1's multi_logloss: 0.157268\n",
      "Early stopping, best iteration is:\n",
      "[91]\ttraining's multi_logloss: 0.0919224\tvalid_1's multi_logloss: 0.157208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156804:  65%|######5   | 13/20 [07:30<03:47, 32.44s/it][I 2020-10-06 15:36:36,149] Trial 19 finished with value: 0.15720787924794113 and parameters: {'num_leaves': 255}. Best is trial 9 with value: 0.15680387039922558.\n",
      "num_leaves, val_score: 0.156804:  65%|######5   | 13/20 [07:30<03:47, 32.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.106282\tvalid_1's multi_logloss: 0.157343\n",
      "Early stopping, best iteration is:\n",
      "[105]\ttraining's multi_logloss: 0.104623\tvalid_1's multi_logloss: 0.157278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156804:  70%|#######   | 14/20 [08:03<03:16, 32.71s/it][I 2020-10-06 15:37:09,494] Trial 20 finished with value: 0.15727805614773113 and parameters: {'num_leaves': 160}. Best is trial 9 with value: 0.15680387039922558.\n",
      "num_leaves, val_score: 0.156804:  70%|#######   | 14/20 [08:03<03:16, 32.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.101891\tvalid_1's multi_logloss: 0.157109\n",
      "Early stopping, best iteration is:\n",
      "[104]\ttraining's multi_logloss: 0.100361\tvalid_1's multi_logloss: 0.157087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156804:  75%|#######5  | 15/20 [08:40<02:49, 33.94s/it][I 2020-10-06 15:37:46,283] Trial 21 finished with value: 0.15708659945280412 and parameters: {'num_leaves': 182}. Best is trial 9 with value: 0.15680387039922558.\n",
      "num_leaves, val_score: 0.156804:  75%|#######5  | 15/20 [08:40<02:49, 33.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0961535\tvalid_1's multi_logloss: 0.157202\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's multi_logloss: 0.0989763\tvalid_1's multi_logloss: 0.157168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156804:  80%|########  | 16/20 [09:20<02:22, 35.64s/it][I 2020-10-06 15:38:25,905] Trial 22 finished with value: 0.15716778097645998 and parameters: {'num_leaves': 213}. Best is trial 9 with value: 0.15680387039922558.\n",
      "num_leaves, val_score: 0.156804:  80%|########  | 16/20 [09:20<02:22, 35.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081324 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.103018\tvalid_1's multi_logloss: 0.157055\n",
      "Early stopping, best iteration is:\n",
      "[96]\ttraining's multi_logloss: 0.104372\tvalid_1's multi_logloss: 0.157046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156804:  85%|########5 | 17/20 [09:52<01:43, 34.63s/it][I 2020-10-06 15:38:58,190] Trial 23 finished with value: 0.15704649597686554 and parameters: {'num_leaves': 178}. Best is trial 9 with value: 0.15680387039922558.\n",
      "num_leaves, val_score: 0.156804:  85%|########5 | 17/20 [09:52<01:43, 34.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[89]\ttraining's multi_logloss: 0.0972762\tvalid_1's multi_logloss: 0.157356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156804:  90%|######### | 18/20 [10:26<01:09, 34.51s/it][I 2020-10-06 15:39:32,401] Trial 24 finished with value: 0.1573555521598015 and parameters: {'num_leaves': 230}. Best is trial 9 with value: 0.15680387039922558.\n",
      "num_leaves, val_score: 0.156804:  90%|######### | 18/20 [10:26<01:09, 34.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080133 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.115825\tvalid_1's multi_logloss: 0.157226\n",
      "Early stopping, best iteration is:\n",
      "[111]\ttraining's multi_logloss: 0.112565\tvalid_1's multi_logloss: 0.157097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156804:  95%|#########5| 19/20 [11:08<00:36, 36.53s/it][I 2020-10-06 15:40:13,655] Trial 25 finished with value: 0.15709736289324908 and parameters: {'num_leaves': 121}. Best is trial 9 with value: 0.15680387039922558.\n",
      "num_leaves, val_score: 0.156804:  95%|#########5| 19/20 [11:08<00:36, 36.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.167653 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.214287\tvalid_1's multi_logloss: 0.215521\n",
      "[200]\ttraining's multi_logloss: 0.202271\tvalid_1's multi_logloss: 0.204313\n",
      "[300]\ttraining's multi_logloss: 0.19733\tvalid_1's multi_logloss: 0.199804\n",
      "[400]\ttraining's multi_logloss: 0.194366\tvalid_1's multi_logloss: 0.197111\n",
      "[500]\ttraining's multi_logloss: 0.192391\tvalid_1's multi_logloss: 0.195315\n",
      "[600]\ttraining's multi_logloss: 0.190995\tvalid_1's multi_logloss: 0.194063\n",
      "[700]\ttraining's multi_logloss: 0.189969\tvalid_1's multi_logloss: 0.19318\n",
      "[800]\ttraining's multi_logloss: 0.189182\tvalid_1's multi_logloss: 0.192515\n",
      "[900]\ttraining's multi_logloss: 0.188555\tvalid_1's multi_logloss: 0.191968\n",
      "[1000]\ttraining's multi_logloss: 0.188036\tvalid_1's multi_logloss: 0.191545\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's multi_logloss: 0.188036\tvalid_1's multi_logloss: 0.191545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.156804: 100%|##########| 20/20 [13:28<00:00, 67.78s/it][I 2020-10-06 15:42:34,358] Trial 26 finished with value: 0.19154527524911957 and parameters: {'num_leaves': 2}. Best is trial 9 with value: 0.15680387039922558.\n",
      "num_leaves, val_score: 0.156804: 100%|##########| 20/20 [13:28<00:00, 40.44s/it]\n",
      "bagging, val_score: 0.156804:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079726 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0897603\tvalid_1's multi_logloss: 0.158597\n",
      "Early stopping, best iteration is:\n",
      "[94]\ttraining's multi_logloss: 0.092583\tvalid_1's multi_logloss: 0.158581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.156804:  10%|#         | 1/10 [00:37<05:36, 37.40s/it][I 2020-10-06 15:43:11,799] Trial 27 finished with value: 0.15858086690081266 and parameters: {'bagging_fraction': 0.6539619545127894, 'bagging_freq': 1}. Best is trial 27 with value: 0.15858086690081266.\n",
      "bagging, val_score: 0.156804:  10%|#         | 1/10 [00:37<05:36, 37.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0915338\tvalid_1's multi_logloss: 0.1569\n",
      "Early stopping, best iteration is:\n",
      "[97]\ttraining's multi_logloss: 0.0928843\tvalid_1's multi_logloss: 0.156884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.156804:  20%|##        | 2/10 [01:16<05:02, 37.85s/it][I 2020-10-06 15:43:50,709] Trial 28 finished with value: 0.15688353226631738 and parameters: {'bagging_fraction': 0.9763923224038545, 'bagging_freq': 7}. Best is trial 28 with value: 0.15688353226631738.\n",
      "bagging, val_score: 0.156804:  20%|##        | 2/10 [01:16<05:02, 37.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0911914\tvalid_1's multi_logloss: 0.157413\n",
      "Early stopping, best iteration is:\n",
      "[100]\ttraining's multi_logloss: 0.0911914\tvalid_1's multi_logloss: 0.157413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.156804:  30%|###       | 3/10 [02:05<04:49, 41.31s/it][I 2020-10-06 15:44:40,065] Trial 29 finished with value: 0.1574128835043251 and parameters: {'bagging_fraction': 0.999286654290796, 'bagging_freq': 7}. Best is trial 28 with value: 0.15688353226631738.\n",
      "bagging, val_score: 0.156804:  30%|###       | 3/10 [02:05<04:49, 41.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085020 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0913625\tvalid_1's multi_logloss: 0.157107\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttraining's multi_logloss: 0.0902049\tvalid_1's multi_logloss: 0.157084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.156804:  40%|####      | 4/10 [02:47<04:07, 41.31s/it][I 2020-10-06 15:45:21,402] Trial 30 finished with value: 0.1570837696507969 and parameters: {'bagging_fraction': 0.9974014255284737, 'bagging_freq': 7}. Best is trial 28 with value: 0.15688353226631738.\n",
      "bagging, val_score: 0.156804:  40%|####      | 4/10 [02:47<04:07, 41.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082408 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[87]\ttraining's multi_logloss: 0.099448\tvalid_1's multi_logloss: 0.160786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.156804:  50%|#####     | 5/10 [03:26<03:23, 40.70s/it][I 2020-10-06 15:46:00,672] Trial 31 finished with value: 0.16078564896672232 and parameters: {'bagging_fraction': 0.4020233937050774, 'bagging_freq': 4}. Best is trial 28 with value: 0.15688353226631738.\n",
      "bagging, val_score: 0.156804:  50%|#####     | 5/10 [03:26<03:23, 40.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.090329\tvalid_1's multi_logloss: 0.157518\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's multi_logloss: 0.0940053\tvalid_1's multi_logloss: 0.157379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.156804:  60%|######    | 6/10 [04:03<02:38, 39.64s/it][I 2020-10-06 15:46:37,820] Trial 32 finished with value: 0.15737942419403222 and parameters: {'bagging_fraction': 0.7901653963484876, 'bagging_freq': 5}. Best is trial 28 with value: 0.15688353226631738.\n",
      "bagging, val_score: 0.156804:  60%|######    | 6/10 [04:03<02:38, 39.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083816 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[89]\ttraining's multi_logloss: 0.0951603\tvalid_1's multi_logloss: 0.157553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.156804:  70%|#######   | 7/10 [04:34<01:51, 37.08s/it][I 2020-10-06 15:47:08,931] Trial 33 finished with value: 0.15755289264242384 and parameters: {'bagging_fraction': 0.7730688269783044, 'bagging_freq': 1}. Best is trial 28 with value: 0.15688353226631738.\n",
      "bagging, val_score: 0.156804:  70%|#######   | 7/10 [04:34<01:51, 37.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084754 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[85]\ttraining's multi_logloss: 0.0991076\tvalid_1's multi_logloss: 0.159315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.156804:  80%|########  | 8/10 [05:11<01:14, 37.08s/it][I 2020-10-06 15:47:46,011] Trial 34 finished with value: 0.15931474972514473 and parameters: {'bagging_fraction': 0.567371600796335, 'bagging_freq': 6}. Best is trial 28 with value: 0.15688353226631738.\n",
      "bagging, val_score: 0.156804:  80%|########  | 8/10 [05:11<01:14, 37.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0909926\tvalid_1's multi_logloss: 0.157464\n",
      "Early stopping, best iteration is:\n",
      "[91]\ttraining's multi_logloss: 0.0948525\tvalid_1's multi_logloss: 0.157417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.156804:  90%|######### | 9/10 [05:58<00:39, 39.95s/it][I 2020-10-06 15:48:32,666] Trial 35 finished with value: 0.1574165951191167 and parameters: {'bagging_fraction': 0.8972925408802401, 'bagging_freq': 3}. Best is trial 28 with value: 0.15688353226631738.\n",
      "bagging, val_score: 0.156804:  90%|######### | 9/10 [05:58<00:39, 39.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[85]\ttraining's multi_logloss: 0.100256\tvalid_1's multi_logloss: 0.160526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.156804: 100%|##########| 10/10 [06:34<00:00, 38.73s/it][I 2020-10-06 15:49:08,558] Trial 36 finished with value: 0.16052581380856532 and parameters: {'bagging_fraction': 0.4003111613900376, 'bagging_freq': 3}. Best is trial 28 with value: 0.15688353226631738.\n",
      "bagging, val_score: 0.156804: 100%|##########| 10/10 [06:34<00:00, 39.42s/it]\n",
      "feature_fraction_stage2, val_score: 0.156804:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102116 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0907285\tvalid_1's multi_logloss: 0.156706\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's multi_logloss: 0.0914043\tvalid_1's multi_logloss: 0.156658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.156658:  17%|#6        | 1/6 [00:27<02:19, 27.86s/it][I 2020-10-06 15:49:36,451] Trial 37 finished with value: 0.15665757486065057 and parameters: {'feature_fraction': 0.6799999999999999}. Best is trial 37 with value: 0.15665757486065057.\n",
      "feature_fraction_stage2, val_score: 0.156658:  17%|#6        | 1/6 [00:27<02:19, 27.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.091673\tvalid_1's multi_logloss: 0.156847\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttraining's multi_logloss: 0.0904789\tvalid_1's multi_logloss: 0.156804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.156658:  33%|###3      | 2/6 [00:54<01:50, 27.58s/it][I 2020-10-06 15:50:03,398] Trial 38 finished with value: 0.15680387039922558 and parameters: {'feature_fraction': 0.616}. Best is trial 37 with value: 0.15665757486065057.\n",
      "feature_fraction_stage2, val_score: 0.156658:  33%|###3      | 2/6 [00:54<01:50, 27.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0909994\tvalid_1's multi_logloss: 0.157037\n",
      "Early stopping, best iteration is:\n",
      "[106]\ttraining's multi_logloss: 0.0889755\tvalid_1's multi_logloss: 0.156981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.156658:  50%|#####     | 3/6 [01:22<01:23, 27.75s/it][I 2020-10-06 15:50:31,525] Trial 39 finished with value: 0.15698077248289216 and parameters: {'feature_fraction': 0.6479999999999999}. Best is trial 37 with value: 0.15665757486065057.\n",
      "feature_fraction_stage2, val_score: 0.156658:  50%|#####     | 3/6 [01:23<01:23, 27.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075507 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0923607\tvalid_1's multi_logloss: 0.157151\n",
      "Early stopping, best iteration is:\n",
      "[100]\ttraining's multi_logloss: 0.0923607\tvalid_1's multi_logloss: 0.157151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.156658:  67%|######6   | 4/6 [01:49<00:54, 27.29s/it][I 2020-10-06 15:50:57,743] Trial 40 finished with value: 0.15715089881518987 and parameters: {'feature_fraction': 0.52}. Best is trial 37 with value: 0.15665757486065057.\n",
      "feature_fraction_stage2, val_score: 0.156658:  67%|######6   | 4/6 [01:49<00:54, 27.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0920963\tvalid_1's multi_logloss: 0.156964\n",
      "Early stopping, best iteration is:\n",
      "[104]\ttraining's multi_logloss: 0.0905273\tvalid_1's multi_logloss: 0.156938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.156658:  83%|########3 | 5/6 [02:19<00:28, 28.28s/it][I 2020-10-06 15:51:28,342] Trial 41 finished with value: 0.1569384652781081 and parameters: {'feature_fraction': 0.552}. Best is trial 37 with value: 0.15665757486065057.\n",
      "feature_fraction_stage2, val_score: 0.156658:  83%|########3 | 5/6 [02:19<00:28, 28.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.110947 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.091673\tvalid_1's multi_logloss: 0.156847\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttraining's multi_logloss: 0.0904789\tvalid_1's multi_logloss: 0.156804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.156658: 100%|##########| 6/6 [02:55<00:00, 30.65s/it][I 2020-10-06 15:52:04,527] Trial 42 finished with value: 0.15680387039922558 and parameters: {'feature_fraction': 0.584}. Best is trial 37 with value: 0.15665757486065057.\n",
      "feature_fraction_stage2, val_score: 0.156658: 100%|##########| 6/6 [02:55<00:00, 29.32s/it]\n",
      "regularization_factors, val_score: 0.156658:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0906069\tvalid_1's multi_logloss: 0.156627\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's multi_logloss: 0.0915183\tvalid_1's multi_logloss: 0.156535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:   5%|5         | 1/20 [00:33<10:28, 33.07s/it][I 2020-10-06 15:52:37,622] Trial 43 finished with value: 0.1565346545850017 and parameters: {'lambda_l1': 7.700053421427285e-06, 'lambda_l2': 5.626169212347464e-05}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:   5%|5         | 1/20 [00:33<10:28, 33.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082050 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0906274\tvalid_1's multi_logloss: 0.156859\n",
      "Early stopping, best iteration is:\n",
      "[93]\ttraining's multi_logloss: 0.0935601\tvalid_1's multi_logloss: 0.156763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:  10%|#         | 2/20 [01:04<09:47, 32.63s/it][I 2020-10-06 15:53:09,241] Trial 44 finished with value: 0.15676340039602044 and parameters: {'lambda_l1': 2.6298591849918232e-06, 'lambda_l2': 3.2208178033716364e-05}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:  10%|#         | 2/20 [01:04<09:47, 32.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082767 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0906275\tvalid_1's multi_logloss: 0.156859\n",
      "Early stopping, best iteration is:\n",
      "[93]\ttraining's multi_logloss: 0.0935602\tvalid_1's multi_logloss: 0.156763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:  15%|#5        | 3/20 [01:37<09:16, 32.75s/it][I 2020-10-06 15:53:42,259] Trial 45 finished with value: 0.15676339339317108 and parameters: {'lambda_l1': 4.182693990680983e-06, 'lambda_l2': 4.086413068780176e-05}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:  15%|#5        | 3/20 [01:37<09:16, 32.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0906275\tvalid_1's multi_logloss: 0.156859\n",
      "Early stopping, best iteration is:\n",
      "[93]\ttraining's multi_logloss: 0.0935602\tvalid_1's multi_logloss: 0.156763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:  20%|##        | 4/20 [02:10<08:43, 32.74s/it][I 2020-10-06 15:54:14,990] Trial 46 finished with value: 0.15676339299136338 and parameters: {'lambda_l1': 3.3902680326104685e-06, 'lambda_l2': 4.2195608129804806e-05}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:  20%|##        | 4/20 [02:10<08:43, 32.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089262 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0903162\tvalid_1's multi_logloss: 0.157052\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's multi_logloss: 0.0911304\tvalid_1's multi_logloss: 0.156987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:  25%|##5       | 5/20 [02:42<08:07, 32.50s/it][I 2020-10-06 15:54:46,925] Trial 47 finished with value: 0.15698673258152795 and parameters: {'lambda_l1': 3.5615974668962767e-06, 'lambda_l2': 6.251205457622922e-05}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:  25%|##5       | 5/20 [02:42<08:07, 32.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0906276\tvalid_1's multi_logloss: 0.156859\n",
      "Early stopping, best iteration is:\n",
      "[93]\ttraining's multi_logloss: 0.0935602\tvalid_1's multi_logloss: 0.156763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:  30%|###       | 6/20 [03:12<07:26, 31.87s/it][I 2020-10-06 15:55:17,314] Trial 48 finished with value: 0.15676340162094907 and parameters: {'lambda_l1': 3.59477495753407e-06, 'lambda_l2': 4.395053292038933e-05}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:  30%|###       | 6/20 [03:12<07:26, 31.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072617 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0906275\tvalid_1's multi_logloss: 0.156859\n",
      "Early stopping, best iteration is:\n",
      "[93]\ttraining's multi_logloss: 0.0935602\tvalid_1's multi_logloss: 0.156763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:  35%|###5      | 7/20 [03:46<07:02, 32.53s/it][I 2020-10-06 15:55:51,388] Trial 49 finished with value: 0.15676340478986722 and parameters: {'lambda_l1': 5.1605735875721735e-06, 'lambda_l2': 3.787189400485706e-05}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:  35%|###5      | 7/20 [03:46<07:02, 32.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0906274\tvalid_1's multi_logloss: 0.156859\n",
      "Early stopping, best iteration is:\n",
      "[93]\ttraining's multi_logloss: 0.0935601\tvalid_1's multi_logloss: 0.156763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:  40%|####      | 8/20 [04:22<06:41, 33.42s/it][I 2020-10-06 15:56:26,884] Trial 50 finished with value: 0.1567634108527408 and parameters: {'lambda_l1': 3.7438573411164865e-06, 'lambda_l2': 3.0455151262396264e-05}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:  40%|####      | 8/20 [04:22<06:41, 33.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111603 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0906068\tvalid_1's multi_logloss: 0.156627\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's multi_logloss: 0.0915182\tvalid_1's multi_logloss: 0.156535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:  45%|####5     | 9/20 [04:53<06:01, 32.84s/it][I 2020-10-06 15:56:58,371] Trial 51 finished with value: 0.15653482525071524 and parameters: {'lambda_l1': 2.7257000701031737e-06, 'lambda_l2': 5.188333763412821e-05}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:  45%|####5     | 9/20 [04:53<06:01, 32.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084919 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0906068\tvalid_1's multi_logloss: 0.156627\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's multi_logloss: 0.0915182\tvalid_1's multi_logloss: 0.156535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:  50%|#####     | 10/20 [05:31<05:42, 34.21s/it][I 2020-10-06 15:57:35,787] Trial 52 finished with value: 0.15653482689030607 and parameters: {'lambda_l1': 2.1821396401064746e-06, 'lambda_l2': 5.0481398162402835e-05}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:  50%|#####     | 10/20 [05:31<05:42, 34.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.108701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttraining's multi_logloss: 0.0905884\tvalid_1's multi_logloss: 0.157143\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[96]\ttraining's multi_logloss: 0.0921355\tvalid_1's multi_logloss: 0.157078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:  55%|#####5    | 11/20 [06:13<05:28, 36.49s/it][I 2020-10-06 15:58:17,607] Trial 53 finished with value: 0.15707753433313604 and parameters: {'lambda_l1': 3.973337897423386e-07, 'lambda_l2': 0.009910978493804914}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:  55%|#####5    | 11/20 [06:13<05:28, 36.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125540 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[88]\ttraining's multi_logloss: 0.0957456\tvalid_1's multi_logloss: 0.156812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:  60%|######    | 12/20 [06:52<04:58, 37.25s/it][I 2020-10-06 15:58:56,631] Trial 54 finished with value: 0.15681197869912603 and parameters: {'lambda_l1': 0.0019930076800397586, 'lambda_l2': 2.337192655451412e-07}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:  60%|######    | 12/20 [06:52<04:58, 37.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072817 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[85]\ttraining's multi_logloss: 0.0970747\tvalid_1's multi_logloss: 0.157423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:  65%|######5   | 13/20 [07:24<04:09, 35.66s/it][I 2020-10-06 15:59:28,569] Trial 55 finished with value: 0.1574228856235996 and parameters: {'lambda_l1': 8.209064479765404e-05, 'lambda_l2': 0.00133213287089073}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:  65%|######5   | 13/20 [07:24<04:09, 35.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0907285\tvalid_1's multi_logloss: 0.156706\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's multi_logloss: 0.0914043\tvalid_1's multi_logloss: 0.156658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:  70%|#######   | 14/20 [07:53<03:23, 33.92s/it][I 2020-10-06 15:59:58,425] Trial 56 finished with value: 0.15665757393404162 and parameters: {'lambda_l1': 3.031087431460285e-08, 'lambda_l2': 1.0551608194360948e-06}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:  70%|#######   | 14/20 [07:53<03:23, 33.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071581 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0907285\tvalid_1's multi_logloss: 0.156706\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's multi_logloss: 0.0914043\tvalid_1's multi_logloss: 0.156658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:  75%|#######5  | 15/20 [08:24<02:44, 32.85s/it][I 2020-10-06 16:00:28,785] Trial 57 finished with value: 0.1566575744425873 and parameters: {'lambda_l1': 2.051183919795635e-08, 'lambda_l2': 4.6991261353684615e-07}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:  75%|#######5  | 15/20 [08:24<02:44, 32.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0907285\tvalid_1's multi_logloss: 0.156706\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's multi_logloss: 0.0914043\tvalid_1's multi_logloss: 0.156658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:  80%|########  | 16/20 [08:54<02:08, 32.15s/it][I 2020-10-06 16:00:59,311] Trial 58 finished with value: 0.15665757477796652 and parameters: {'lambda_l1': 2.3495566778418965e-08, 'lambda_l2': 7.065747110815646e-08}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:  80%|########  | 16/20 [08:54<02:08, 32.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069867 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0907285\tvalid_1's multi_logloss: 0.156706\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's multi_logloss: 0.0914043\tvalid_1's multi_logloss: 0.156657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:  85%|########5 | 17/20 [09:24<01:34, 31.55s/it][I 2020-10-06 16:01:29,452] Trial 59 finished with value: 0.15665741378625664 and parameters: {'lambda_l1': 1.1565267116200953e-08, 'lambda_l2': 1.0166957051684706e-07}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:  85%|########5 | 17/20 [09:24<01:34, 31.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0907285\tvalid_1's multi_logloss: 0.156706\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's multi_logloss: 0.0914043\tvalid_1's multi_logloss: 0.156658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:  90%|######### | 18/20 [09:57<01:04, 32.01s/it][I 2020-10-06 16:02:02,528] Trial 60 finished with value: 0.1566575742857229 and parameters: {'lambda_l1': 1.0369862465908044e-08, 'lambda_l2': 6.690367939255532e-07}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:  90%|######### | 18/20 [09:57<01:04, 32.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0907285\tvalid_1's multi_logloss: 0.156706\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's multi_logloss: 0.0914043\tvalid_1's multi_logloss: 0.156658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535:  95%|#########5| 19/20 [10:27<00:31, 31.26s/it][I 2020-10-06 16:02:32,053] Trial 61 finished with value: 0.15665757079656567 and parameters: {'lambda_l1': 1.3357918445859945e-08, 'lambda_l2': 4.391144097678618e-07}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535:  95%|#########5| 19/20 [10:27<00:31, 31.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.143612 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0907285\tvalid_1's multi_logloss: 0.156706\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's multi_logloss: 0.0914043\tvalid_1's multi_logloss: 0.156658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.156535: 100%|##########| 20/20 [11:01<00:00, 32.04s/it][I 2020-10-06 16:03:05,910] Trial 62 finished with value: 0.1566575738937913 and parameters: {'lambda_l1': 1.26874695420072e-08, 'lambda_l2': 1.1263512162365619e-06}. Best is trial 43 with value: 0.1565346545850017.\n",
      "regularization_factors, val_score: 0.156535: 100%|##########| 20/20 [11:01<00:00, 33.07s/it]\n",
      "min_data_in_leaf, val_score: 0.156535:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067938 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttraining's multi_logloss: 0.0935844\tvalid_1's multi_logloss: 0.157501\n",
      "Early stopping, best iteration is:\n",
      "[90]\ttraining's multi_logloss: 0.0980056\tvalid_1's multi_logloss: 0.157411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.156535:  20%|##        | 1/5 [00:31<02:06, 31.62s/it][I 2020-10-06 16:03:37,557] Trial 63 finished with value: 0.15741094845179504 and parameters: {'min_child_samples': 50}. Best is trial 63 with value: 0.15741094845179504.\n",
      "min_data_in_leaf, val_score: 0.156535:  20%|##        | 1/5 [00:31<02:06, 31.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068632 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0887112\tvalid_1's multi_logloss: 0.157051\n",
      "Early stopping, best iteration is:\n",
      "[102]\ttraining's multi_logloss: 0.0879648\tvalid_1's multi_logloss: 0.157047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.156535:  40%|####      | 2/5 [01:02<01:33, 31.32s/it][I 2020-10-06 16:04:08,182] Trial 64 finished with value: 0.1570468802959867 and parameters: {'min_child_samples': 10}. Best is trial 64 with value: 0.1570468802959867.\n",
      "min_data_in_leaf, val_score: 0.156535:  40%|####      | 2/5 [01:02<01:33, 31.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072622 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0914125\tvalid_1's multi_logloss: 0.156809\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[97]\ttraining's multi_logloss: 0.0927623\tvalid_1's multi_logloss: 0.156766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.156535:  60%|######    | 3/5 [01:33<01:02, 31.22s/it][I 2020-10-06 16:04:39,172] Trial 65 finished with value: 0.15676571735921305 and parameters: {'min_child_samples': 25}. Best is trial 65 with value: 0.15676571735921305.\n",
      "min_data_in_leaf, val_score: 0.156535:  60%|######    | 3/5 [01:33<01:02, 31.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075096 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttraining's multi_logloss: 0.0973487\tvalid_1's multi_logloss: 0.157193\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[99]\ttraining's multi_logloss: 0.0977617\tvalid_1's multi_logloss: 0.157152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.156535:  80%|########  | 4/5 [02:04<00:31, 31.26s/it][I 2020-10-06 16:05:10,528] Trial 66 finished with value: 0.15715247869163826 and parameters: {'min_child_samples': 100}. Best is trial 65 with value: 0.15676571735921305.\n",
      "min_data_in_leaf, val_score: 0.156535:  80%|########  | 4/5 [02:04<00:31, 31.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070620 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 256000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.980965\n",
      "[LightGBM] [Info] Start training from score -2.012659\n",
      "[LightGBM] [Info] Start training from score -0.710460\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's multi_logloss: 0.0875395\tvalid_1's multi_logloss: 0.157565\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's multi_logloss: 0.090745\tvalid_1's multi_logloss: 0.157452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.156535: 100%|##########| 5/5 [02:31<00:00, 29.96s/it][I 2020-10-06 16:05:37,459] Trial 67 finished with value: 0.15745171506899577 and parameters: {'min_child_samples': 5}. Best is trial 65 with value: 0.15676571735921305.\n",
      "min_data_in_leaf, val_score: 0.156535: 100%|##########| 5/5 [02:31<00:00, 30.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'objective': 'multiclass', 'metric': 'multi_logloss', 'num_class': 3, 'lambda_l1': 7.700053421427285e-06, 'lambda_l2': 5.626169212347464e-05, 'random_state': 42, 'n_jobs': -1, 'feature_pre_filter': False, 'bagging_freq': 1, 'num_leaves': 237, 'feature_fraction': 0.6799999999999999, 'bagging_fraction': 1.0, 'min_child_samples': 20, 'num_iterations': 1000, 'early_stopping_round': 10}\n",
      "  Accuracy = 0.9345625\n",
      "  Params: \n",
      "    objective: multiclass\n",
      "    metric: multi_logloss\n",
      "    num_class: 3\n",
      "    lambda_l1: 7.700053421427285e-06\n",
      "    lambda_l2: 5.626169212347464e-05\n",
      "    random_state: 42\n",
      "    n_jobs: -1\n",
      "    feature_pre_filter: False\n",
      "    bagging_freq: 1\n",
      "    num_leaves: 237\n",
      "    feature_fraction: 0.6799999999999999\n",
      "    bagging_fraction: 1.0\n",
      "    min_child_samples: 20\n",
      "    num_iterations: 1000\n",
      "    early_stopping_round: 10\n"
     ]
    }
   ],
   "source": [
    "dtrain = lgb.Dataset(X_trn, label=y_trn)\n",
    "dval = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "model = lgb.train(params, dtrain, valid_sets=[dtrain, dval], \n",
    "                  verbose_eval=100, early_stopping_rounds=10)\n",
    "\n",
    "prediction = np.argmax(model.predict(X_val, num_iteration=model.best_iteration), \n",
    "                       axis=1)\n",
    "accuracy = accuracy_score(y_val, prediction)\n",
    "\n",
    "params = model.params\n",
    "print(\"Best params:\", params)\n",
    "print(\"  Accuracy = {}\".format(accuracy))\n",
    "print(\"  Params: \")\n",
    "for key, value in params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T07:06:40.597147Z",
     "start_time": "2020-10-06T07:06:39.590722Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T07:09:20.449309Z",
     "start_time": "2020-10-06T07:06:41.504293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model for CV #1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6799999999999999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6799999999999999\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.700053421427285e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.700053421427285e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.626169212347464e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.626169212347464e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[1]\tvalid_0's multi_logloss: 0.848654\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.741361\n",
      "[3]\tvalid_0's multi_logloss: 0.656453\n",
      "[4]\tvalid_0's multi_logloss: 0.582812\n",
      "[5]\tvalid_0's multi_logloss: 0.524435\n",
      "[6]\tvalid_0's multi_logloss: 0.474217\n",
      "[7]\tvalid_0's multi_logloss: 0.433994\n",
      "[8]\tvalid_0's multi_logloss: 0.399751\n",
      "[9]\tvalid_0's multi_logloss: 0.369277\n",
      "[10]\tvalid_0's multi_logloss: 0.344205\n",
      "[11]\tvalid_0's multi_logloss: 0.321165\n",
      "[12]\tvalid_0's multi_logloss: 0.302451\n",
      "[13]\tvalid_0's multi_logloss: 0.286221\n",
      "[14]\tvalid_0's multi_logloss: 0.271502\n",
      "[15]\tvalid_0's multi_logloss: 0.260121\n",
      "[16]\tvalid_0's multi_logloss: 0.249705\n",
      "[17]\tvalid_0's multi_logloss: 0.239118\n",
      "[18]\tvalid_0's multi_logloss: 0.229416\n",
      "[19]\tvalid_0's multi_logloss: 0.221752\n",
      "[20]\tvalid_0's multi_logloss: 0.21648\n",
      "[21]\tvalid_0's multi_logloss: 0.209505\n",
      "[22]\tvalid_0's multi_logloss: 0.20346\n",
      "[23]\tvalid_0's multi_logloss: 0.199417\n",
      "[24]\tvalid_0's multi_logloss: 0.195219\n",
      "[25]\tvalid_0's multi_logloss: 0.191823\n",
      "[26]\tvalid_0's multi_logloss: 0.187847\n",
      "[27]\tvalid_0's multi_logloss: 0.185171\n",
      "[28]\tvalid_0's multi_logloss: 0.182579\n",
      "[29]\tvalid_0's multi_logloss: 0.180886\n",
      "[30]\tvalid_0's multi_logloss: 0.178725\n",
      "[31]\tvalid_0's multi_logloss: 0.176262\n",
      "[32]\tvalid_0's multi_logloss: 0.174618\n",
      "[33]\tvalid_0's multi_logloss: 0.172654\n",
      "[34]\tvalid_0's multi_logloss: 0.171622\n",
      "[35]\tvalid_0's multi_logloss: 0.170443\n",
      "[36]\tvalid_0's multi_logloss: 0.169411\n",
      "[37]\tvalid_0's multi_logloss: 0.168178\n",
      "[38]\tvalid_0's multi_logloss: 0.166892\n",
      "[39]\tvalid_0's multi_logloss: 0.165885\n",
      "[40]\tvalid_0's multi_logloss: 0.165326\n",
      "[41]\tvalid_0's multi_logloss: 0.164874\n",
      "[42]\tvalid_0's multi_logloss: 0.163856\n",
      "[43]\tvalid_0's multi_logloss: 0.163004\n",
      "[44]\tvalid_0's multi_logloss: 0.162321\n",
      "[45]\tvalid_0's multi_logloss: 0.161579\n",
      "[46]\tvalid_0's multi_logloss: 0.161279\n",
      "[47]\tvalid_0's multi_logloss: 0.16097\n",
      "[48]\tvalid_0's multi_logloss: 0.16073\n",
      "[49]\tvalid_0's multi_logloss: 0.160346\n",
      "[50]\tvalid_0's multi_logloss: 0.160141\n",
      "[51]\tvalid_0's multi_logloss: 0.159928\n",
      "[52]\tvalid_0's multi_logloss: 0.15968\n",
      "[53]\tvalid_0's multi_logloss: 0.159381\n",
      "[54]\tvalid_0's multi_logloss: 0.159042\n",
      "[55]\tvalid_0's multi_logloss: 0.158601\n",
      "[56]\tvalid_0's multi_logloss: 0.158299\n",
      "[57]\tvalid_0's multi_logloss: 0.158059\n",
      "[58]\tvalid_0's multi_logloss: 0.157716\n",
      "[59]\tvalid_0's multi_logloss: 0.157445\n",
      "[60]\tvalid_0's multi_logloss: 0.157161\n",
      "[61]\tvalid_0's multi_logloss: 0.156945\n",
      "[62]\tvalid_0's multi_logloss: 0.156803\n",
      "[63]\tvalid_0's multi_logloss: 0.156668\n",
      "[64]\tvalid_0's multi_logloss: 0.156472\n",
      "[65]\tvalid_0's multi_logloss: 0.156328\n",
      "[66]\tvalid_0's multi_logloss: 0.156173\n",
      "[67]\tvalid_0's multi_logloss: 0.15604\n",
      "[68]\tvalid_0's multi_logloss: 0.15591\n",
      "[69]\tvalid_0's multi_logloss: 0.155826\n",
      "[70]\tvalid_0's multi_logloss: 0.155778\n",
      "[71]\tvalid_0's multi_logloss: 0.1557\n",
      "[72]\tvalid_0's multi_logloss: 0.155622\n",
      "[73]\tvalid_0's multi_logloss: 0.155548\n",
      "[74]\tvalid_0's multi_logloss: 0.155444\n",
      "[75]\tvalid_0's multi_logloss: 0.155367\n",
      "[76]\tvalid_0's multi_logloss: 0.155296\n",
      "[77]\tvalid_0's multi_logloss: 0.155213\n",
      "[78]\tvalid_0's multi_logloss: 0.155177\n",
      "[79]\tvalid_0's multi_logloss: 0.155131\n",
      "[80]\tvalid_0's multi_logloss: 0.155128\n",
      "[81]\tvalid_0's multi_logloss: 0.155099\n",
      "[82]\tvalid_0's multi_logloss: 0.155087\n",
      "[83]\tvalid_0's multi_logloss: 0.155083\n",
      "[84]\tvalid_0's multi_logloss: 0.155054\n",
      "[85]\tvalid_0's multi_logloss: 0.155029\n",
      "[86]\tvalid_0's multi_logloss: 0.155049\n",
      "[87]\tvalid_0's multi_logloss: 0.155068\n",
      "[88]\tvalid_0's multi_logloss: 0.155059\n",
      "[89]\tvalid_0's multi_logloss: 0.155068\n",
      "[90]\tvalid_0's multi_logloss: 0.155031\n",
      "[91]\tvalid_0's multi_logloss: 0.155067\n",
      "[92]\tvalid_0's multi_logloss: 0.155074\n",
      "[93]\tvalid_0's multi_logloss: 0.155084\n",
      "[94]\tvalid_0's multi_logloss: 0.155079\n",
      "[95]\tvalid_0's multi_logloss: 0.155114\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid_0's multi_logloss: 0.155029\n",
      "training model for CV #2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6799999999999999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6799999999999999\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.700053421427285e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.700053421427285e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.626169212347464e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.626169212347464e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[1]\tvalid_0's multi_logloss: 0.849234\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.742171\n",
      "[3]\tvalid_0's multi_logloss: 0.657386\n",
      "[4]\tvalid_0's multi_logloss: 0.58375\n",
      "[5]\tvalid_0's multi_logloss: 0.525515\n",
      "[6]\tvalid_0's multi_logloss: 0.475435\n",
      "[7]\tvalid_0's multi_logloss: 0.435146\n",
      "[8]\tvalid_0's multi_logloss: 0.401021\n",
      "[9]\tvalid_0's multi_logloss: 0.370627\n",
      "[10]\tvalid_0's multi_logloss: 0.345576\n",
      "[11]\tvalid_0's multi_logloss: 0.322753\n",
      "[12]\tvalid_0's multi_logloss: 0.30408\n",
      "[13]\tvalid_0's multi_logloss: 0.287884\n",
      "[14]\tvalid_0's multi_logloss: 0.273201\n",
      "[15]\tvalid_0's multi_logloss: 0.261824\n",
      "[16]\tvalid_0's multi_logloss: 0.251409\n",
      "[17]\tvalid_0's multi_logloss: 0.240816\n",
      "[18]\tvalid_0's multi_logloss: 0.231111\n",
      "[19]\tvalid_0's multi_logloss: 0.223407\n",
      "[20]\tvalid_0's multi_logloss: 0.21802\n",
      "[21]\tvalid_0's multi_logloss: 0.211085\n",
      "[22]\tvalid_0's multi_logloss: 0.205038\n",
      "[23]\tvalid_0's multi_logloss: 0.201015\n",
      "[24]\tvalid_0's multi_logloss: 0.196737\n",
      "[25]\tvalid_0's multi_logloss: 0.193295\n",
      "[26]\tvalid_0's multi_logloss: 0.189302\n",
      "[27]\tvalid_0's multi_logloss: 0.186699\n",
      "[28]\tvalid_0's multi_logloss: 0.184127\n",
      "[29]\tvalid_0's multi_logloss: 0.182387\n",
      "[30]\tvalid_0's multi_logloss: 0.180171\n",
      "[31]\tvalid_0's multi_logloss: 0.177635\n",
      "[32]\tvalid_0's multi_logloss: 0.1759\n",
      "[33]\tvalid_0's multi_logloss: 0.173828\n",
      "[34]\tvalid_0's multi_logloss: 0.172751\n",
      "[35]\tvalid_0's multi_logloss: 0.171605\n",
      "[36]\tvalid_0's multi_logloss: 0.170534\n",
      "[37]\tvalid_0's multi_logloss: 0.169238\n",
      "[38]\tvalid_0's multi_logloss: 0.167982\n",
      "[39]\tvalid_0's multi_logloss: 0.166988\n",
      "[40]\tvalid_0's multi_logloss: 0.166443\n",
      "[41]\tvalid_0's multi_logloss: 0.165946\n",
      "[42]\tvalid_0's multi_logloss: 0.164909\n",
      "[43]\tvalid_0's multi_logloss: 0.164084\n",
      "[44]\tvalid_0's multi_logloss: 0.163404\n",
      "[45]\tvalid_0's multi_logloss: 0.162607\n",
      "[46]\tvalid_0's multi_logloss: 0.16232\n",
      "[47]\tvalid_0's multi_logloss: 0.162017\n",
      "[48]\tvalid_0's multi_logloss: 0.161714\n",
      "[49]\tvalid_0's multi_logloss: 0.161372\n",
      "[50]\tvalid_0's multi_logloss: 0.161158\n",
      "[51]\tvalid_0's multi_logloss: 0.160962\n",
      "[52]\tvalid_0's multi_logloss: 0.160623\n",
      "[53]\tvalid_0's multi_logloss: 0.160429\n",
      "[54]\tvalid_0's multi_logloss: 0.16013\n",
      "[55]\tvalid_0's multi_logloss: 0.159742\n",
      "[56]\tvalid_0's multi_logloss: 0.159415\n",
      "[57]\tvalid_0's multi_logloss: 0.159119\n",
      "[58]\tvalid_0's multi_logloss: 0.158772\n",
      "[59]\tvalid_0's multi_logloss: 0.158507\n",
      "[60]\tvalid_0's multi_logloss: 0.158206\n",
      "[61]\tvalid_0's multi_logloss: 0.157934\n",
      "[62]\tvalid_0's multi_logloss: 0.157748\n",
      "[63]\tvalid_0's multi_logloss: 0.157573\n",
      "[64]\tvalid_0's multi_logloss: 0.157376\n",
      "[65]\tvalid_0's multi_logloss: 0.157232\n",
      "[66]\tvalid_0's multi_logloss: 0.157062\n",
      "[67]\tvalid_0's multi_logloss: 0.156982\n",
      "[68]\tvalid_0's multi_logloss: 0.1569\n",
      "[69]\tvalid_0's multi_logloss: 0.156859\n",
      "[70]\tvalid_0's multi_logloss: 0.156867\n",
      "[71]\tvalid_0's multi_logloss: 0.156732\n",
      "[72]\tvalid_0's multi_logloss: 0.156615\n",
      "[73]\tvalid_0's multi_logloss: 0.156555\n",
      "[74]\tvalid_0's multi_logloss: 0.156481\n",
      "[75]\tvalid_0's multi_logloss: 0.156409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76]\tvalid_0's multi_logloss: 0.156354\n",
      "[77]\tvalid_0's multi_logloss: 0.156281\n",
      "[78]\tvalid_0's multi_logloss: 0.156259\n",
      "[79]\tvalid_0's multi_logloss: 0.156205\n",
      "[80]\tvalid_0's multi_logloss: 0.156167\n",
      "[81]\tvalid_0's multi_logloss: 0.156111\n",
      "[82]\tvalid_0's multi_logloss: 0.156091\n",
      "[83]\tvalid_0's multi_logloss: 0.156088\n",
      "[84]\tvalid_0's multi_logloss: 0.156066\n",
      "[85]\tvalid_0's multi_logloss: 0.156012\n",
      "[86]\tvalid_0's multi_logloss: 0.156015\n",
      "[87]\tvalid_0's multi_logloss: 0.155987\n",
      "[88]\tvalid_0's multi_logloss: 0.155972\n",
      "[89]\tvalid_0's multi_logloss: 0.155977\n",
      "[90]\tvalid_0's multi_logloss: 0.155979\n",
      "[91]\tvalid_0's multi_logloss: 0.155972\n",
      "[92]\tvalid_0's multi_logloss: 0.155995\n",
      "[93]\tvalid_0's multi_logloss: 0.156002\n",
      "[94]\tvalid_0's multi_logloss: 0.156034\n",
      "[95]\tvalid_0's multi_logloss: 0.156065\n",
      "[96]\tvalid_0's multi_logloss: 0.156053\n",
      "[97]\tvalid_0's multi_logloss: 0.156039\n",
      "[98]\tvalid_0's multi_logloss: 0.156046\n",
      "[99]\tvalid_0's multi_logloss: 0.156043\n",
      "[100]\tvalid_0's multi_logloss: 0.156049\n",
      "[101]\tvalid_0's multi_logloss: 0.156063\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's multi_logloss: 0.155972\n",
      "training model for CV #3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6799999999999999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6799999999999999\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.700053421427285e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.700053421427285e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.626169212347464e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.626169212347464e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[1]\tvalid_0's multi_logloss: 0.849166\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.742153\n",
      "[3]\tvalid_0's multi_logloss: 0.657281\n",
      "[4]\tvalid_0's multi_logloss: 0.583631\n",
      "[5]\tvalid_0's multi_logloss: 0.525471\n",
      "[6]\tvalid_0's multi_logloss: 0.475482\n",
      "[7]\tvalid_0's multi_logloss: 0.435399\n",
      "[8]\tvalid_0's multi_logloss: 0.401219\n",
      "[9]\tvalid_0's multi_logloss: 0.370914\n",
      "[10]\tvalid_0's multi_logloss: 0.345841\n",
      "[11]\tvalid_0's multi_logloss: 0.322931\n",
      "[12]\tvalid_0's multi_logloss: 0.304265\n",
      "[13]\tvalid_0's multi_logloss: 0.287973\n",
      "[14]\tvalid_0's multi_logloss: 0.273218\n",
      "[15]\tvalid_0's multi_logloss: 0.261838\n",
      "[16]\tvalid_0's multi_logloss: 0.251441\n",
      "[17]\tvalid_0's multi_logloss: 0.240895\n",
      "[18]\tvalid_0's multi_logloss: 0.231255\n",
      "[19]\tvalid_0's multi_logloss: 0.223623\n",
      "[20]\tvalid_0's multi_logloss: 0.218343\n",
      "[21]\tvalid_0's multi_logloss: 0.211476\n",
      "[22]\tvalid_0's multi_logloss: 0.205449\n",
      "[23]\tvalid_0's multi_logloss: 0.20141\n",
      "[24]\tvalid_0's multi_logloss: 0.197135\n",
      "[25]\tvalid_0's multi_logloss: 0.193785\n",
      "[26]\tvalid_0's multi_logloss: 0.189802\n",
      "[27]\tvalid_0's multi_logloss: 0.187131\n",
      "[28]\tvalid_0's multi_logloss: 0.184612\n",
      "[29]\tvalid_0's multi_logloss: 0.18288\n",
      "[30]\tvalid_0's multi_logloss: 0.180749\n",
      "[31]\tvalid_0's multi_logloss: 0.178242\n",
      "[32]\tvalid_0's multi_logloss: 0.176665\n",
      "[33]\tvalid_0's multi_logloss: 0.17461\n",
      "[34]\tvalid_0's multi_logloss: 0.173544\n",
      "[35]\tvalid_0's multi_logloss: 0.172415\n",
      "[36]\tvalid_0's multi_logloss: 0.17136\n",
      "[37]\tvalid_0's multi_logloss: 0.170108\n",
      "[38]\tvalid_0's multi_logloss: 0.168941\n",
      "[39]\tvalid_0's multi_logloss: 0.168045\n",
      "[40]\tvalid_0's multi_logloss: 0.167445\n",
      "[41]\tvalid_0's multi_logloss: 0.167022\n",
      "[42]\tvalid_0's multi_logloss: 0.166064\n",
      "[43]\tvalid_0's multi_logloss: 0.165227\n",
      "[44]\tvalid_0's multi_logloss: 0.164543\n",
      "[45]\tvalid_0's multi_logloss: 0.163776\n",
      "[46]\tvalid_0's multi_logloss: 0.163506\n",
      "[47]\tvalid_0's multi_logloss: 0.163162\n",
      "[48]\tvalid_0's multi_logloss: 0.162854\n",
      "[49]\tvalid_0's multi_logloss: 0.162543\n",
      "[50]\tvalid_0's multi_logloss: 0.162315\n",
      "[51]\tvalid_0's multi_logloss: 0.162113\n",
      "[52]\tvalid_0's multi_logloss: 0.161837\n",
      "[53]\tvalid_0's multi_logloss: 0.161613\n",
      "[54]\tvalid_0's multi_logloss: 0.16134\n",
      "[55]\tvalid_0's multi_logloss: 0.160874\n",
      "[56]\tvalid_0's multi_logloss: 0.160572\n",
      "[57]\tvalid_0's multi_logloss: 0.160229\n",
      "[58]\tvalid_0's multi_logloss: 0.159907\n",
      "[59]\tvalid_0's multi_logloss: 0.159642\n",
      "[60]\tvalid_0's multi_logloss: 0.159425\n",
      "[61]\tvalid_0's multi_logloss: 0.159219\n",
      "[62]\tvalid_0's multi_logloss: 0.159055\n",
      "[63]\tvalid_0's multi_logloss: 0.158877\n",
      "[64]\tvalid_0's multi_logloss: 0.158711\n",
      "[65]\tvalid_0's multi_logloss: 0.15856\n",
      "[66]\tvalid_0's multi_logloss: 0.158351\n",
      "[67]\tvalid_0's multi_logloss: 0.158277\n",
      "[68]\tvalid_0's multi_logloss: 0.15822\n",
      "[69]\tvalid_0's multi_logloss: 0.158196\n",
      "[70]\tvalid_0's multi_logloss: 0.158147\n",
      "[71]\tvalid_0's multi_logloss: 0.158069\n",
      "[72]\tvalid_0's multi_logloss: 0.157988\n",
      "[73]\tvalid_0's multi_logloss: 0.157927\n",
      "[74]\tvalid_0's multi_logloss: 0.157826\n",
      "[75]\tvalid_0's multi_logloss: 0.157772\n",
      "[76]\tvalid_0's multi_logloss: 0.157728\n",
      "[77]\tvalid_0's multi_logloss: 0.157705\n",
      "[78]\tvalid_0's multi_logloss: 0.157645\n",
      "[79]\tvalid_0's multi_logloss: 0.157592\n",
      "[80]\tvalid_0's multi_logloss: 0.157595\n",
      "[81]\tvalid_0's multi_logloss: 0.157552\n",
      "[82]\tvalid_0's multi_logloss: 0.157492\n",
      "[83]\tvalid_0's multi_logloss: 0.157479\n",
      "[84]\tvalid_0's multi_logloss: 0.157492\n",
      "[85]\tvalid_0's multi_logloss: 0.157508\n",
      "[86]\tvalid_0's multi_logloss: 0.157548\n",
      "[87]\tvalid_0's multi_logloss: 0.157562\n",
      "[88]\tvalid_0's multi_logloss: 0.157561\n",
      "[89]\tvalid_0's multi_logloss: 0.157524\n",
      "[90]\tvalid_0's multi_logloss: 0.157563\n",
      "[91]\tvalid_0's multi_logloss: 0.157544\n",
      "[92]\tvalid_0's multi_logloss: 0.15753\n",
      "[93]\tvalid_0's multi_logloss: 0.157537\n",
      "Early stopping, best iteration is:\n",
      "[83]\tvalid_0's multi_logloss: 0.157479\n",
      "training model for CV #4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6799999999999999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6799999999999999\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.700053421427285e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.700053421427285e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.626169212347464e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.626169212347464e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[1]\tvalid_0's multi_logloss: 0.848911\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.741605\n",
      "[3]\tvalid_0's multi_logloss: 0.656753\n",
      "[4]\tvalid_0's multi_logloss: 0.583256\n",
      "[5]\tvalid_0's multi_logloss: 0.525064\n",
      "[6]\tvalid_0's multi_logloss: 0.474931\n",
      "[7]\tvalid_0's multi_logloss: 0.434741\n",
      "[8]\tvalid_0's multi_logloss: 0.400598\n",
      "[9]\tvalid_0's multi_logloss: 0.370215\n",
      "[10]\tvalid_0's multi_logloss: 0.34511\n",
      "[11]\tvalid_0's multi_logloss: 0.322148\n",
      "[12]\tvalid_0's multi_logloss: 0.303516\n",
      "[13]\tvalid_0's multi_logloss: 0.287308\n",
      "[14]\tvalid_0's multi_logloss: 0.272662\n",
      "[15]\tvalid_0's multi_logloss: 0.261362\n",
      "[16]\tvalid_0's multi_logloss: 0.250987\n",
      "[17]\tvalid_0's multi_logloss: 0.24047\n",
      "[18]\tvalid_0's multi_logloss: 0.23083\n",
      "[19]\tvalid_0's multi_logloss: 0.223173\n",
      "[20]\tvalid_0's multi_logloss: 0.217817\n",
      "[21]\tvalid_0's multi_logloss: 0.210863\n",
      "[22]\tvalid_0's multi_logloss: 0.204907\n",
      "[23]\tvalid_0's multi_logloss: 0.200891\n",
      "[24]\tvalid_0's multi_logloss: 0.19664\n",
      "[25]\tvalid_0's multi_logloss: 0.193253\n",
      "[26]\tvalid_0's multi_logloss: 0.18931\n",
      "[27]\tvalid_0's multi_logloss: 0.186641\n",
      "[28]\tvalid_0's multi_logloss: 0.18408\n",
      "[29]\tvalid_0's multi_logloss: 0.182336\n",
      "[30]\tvalid_0's multi_logloss: 0.180128\n",
      "[31]\tvalid_0's multi_logloss: 0.177602\n",
      "[32]\tvalid_0's multi_logloss: 0.175911\n",
      "[33]\tvalid_0's multi_logloss: 0.173872\n",
      "[34]\tvalid_0's multi_logloss: 0.172865\n",
      "[35]\tvalid_0's multi_logloss: 0.171706\n",
      "[36]\tvalid_0's multi_logloss: 0.170703\n",
      "[37]\tvalid_0's multi_logloss: 0.169453\n",
      "[38]\tvalid_0's multi_logloss: 0.168239\n",
      "[39]\tvalid_0's multi_logloss: 0.167243\n",
      "[40]\tvalid_0's multi_logloss: 0.16665\n",
      "[41]\tvalid_0's multi_logloss: 0.166208\n",
      "[42]\tvalid_0's multi_logloss: 0.165202\n",
      "[43]\tvalid_0's multi_logloss: 0.164349\n",
      "[44]\tvalid_0's multi_logloss: 0.163664\n",
      "[45]\tvalid_0's multi_logloss: 0.162899\n",
      "[46]\tvalid_0's multi_logloss: 0.16256\n",
      "[47]\tvalid_0's multi_logloss: 0.162303\n",
      "[48]\tvalid_0's multi_logloss: 0.162043\n",
      "[49]\tvalid_0's multi_logloss: 0.161714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's multi_logloss: 0.161457\n",
      "[51]\tvalid_0's multi_logloss: 0.161228\n",
      "[52]\tvalid_0's multi_logloss: 0.160937\n",
      "[53]\tvalid_0's multi_logloss: 0.160713\n",
      "[54]\tvalid_0's multi_logloss: 0.160475\n",
      "[55]\tvalid_0's multi_logloss: 0.160047\n",
      "[56]\tvalid_0's multi_logloss: 0.159725\n",
      "[57]\tvalid_0's multi_logloss: 0.159396\n",
      "[58]\tvalid_0's multi_logloss: 0.159053\n",
      "[59]\tvalid_0's multi_logloss: 0.158794\n",
      "[60]\tvalid_0's multi_logloss: 0.158508\n",
      "[61]\tvalid_0's multi_logloss: 0.158241\n",
      "[62]\tvalid_0's multi_logloss: 0.158073\n",
      "[63]\tvalid_0's multi_logloss: 0.157894\n",
      "[64]\tvalid_0's multi_logloss: 0.157687\n",
      "[65]\tvalid_0's multi_logloss: 0.157562\n",
      "[66]\tvalid_0's multi_logloss: 0.157347\n",
      "[67]\tvalid_0's multi_logloss: 0.157149\n",
      "[68]\tvalid_0's multi_logloss: 0.157094\n",
      "[69]\tvalid_0's multi_logloss: 0.157026\n",
      "[70]\tvalid_0's multi_logloss: 0.156964\n",
      "[71]\tvalid_0's multi_logloss: 0.156834\n",
      "[72]\tvalid_0's multi_logloss: 0.156734\n",
      "[73]\tvalid_0's multi_logloss: 0.156646\n",
      "[74]\tvalid_0's multi_logloss: 0.156494\n",
      "[75]\tvalid_0's multi_logloss: 0.156479\n",
      "[76]\tvalid_0's multi_logloss: 0.156345\n",
      "[77]\tvalid_0's multi_logloss: 0.15627\n",
      "[78]\tvalid_0's multi_logloss: 0.156194\n",
      "[79]\tvalid_0's multi_logloss: 0.156148\n",
      "[80]\tvalid_0's multi_logloss: 0.156143\n",
      "[81]\tvalid_0's multi_logloss: 0.156085\n",
      "[82]\tvalid_0's multi_logloss: 0.156006\n",
      "[83]\tvalid_0's multi_logloss: 0.155996\n",
      "[84]\tvalid_0's multi_logloss: 0.155998\n",
      "[85]\tvalid_0's multi_logloss: 0.155992\n",
      "[86]\tvalid_0's multi_logloss: 0.155968\n",
      "[87]\tvalid_0's multi_logloss: 0.15598\n",
      "[88]\tvalid_0's multi_logloss: 0.155953\n",
      "[89]\tvalid_0's multi_logloss: 0.155957\n",
      "[90]\tvalid_0's multi_logloss: 0.155957\n",
      "[91]\tvalid_0's multi_logloss: 0.155963\n",
      "[92]\tvalid_0's multi_logloss: 0.155931\n",
      "[93]\tvalid_0's multi_logloss: 0.155933\n",
      "[94]\tvalid_0's multi_logloss: 0.155958\n",
      "[95]\tvalid_0's multi_logloss: 0.155927\n",
      "[96]\tvalid_0's multi_logloss: 0.155954\n",
      "[97]\tvalid_0's multi_logloss: 0.155954\n",
      "[98]\tvalid_0's multi_logloss: 0.155956\n",
      "[99]\tvalid_0's multi_logloss: 0.15594\n",
      "[100]\tvalid_0's multi_logloss: 0.15594\n",
      "[101]\tvalid_0's multi_logloss: 0.155963\n",
      "[102]\tvalid_0's multi_logloss: 0.155982\n",
      "[103]\tvalid_0's multi_logloss: 0.155964\n",
      "[104]\tvalid_0's multi_logloss: 0.155976\n",
      "[105]\tvalid_0's multi_logloss: 0.155953\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid_0's multi_logloss: 0.155927\n",
      "training model for CV #5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6799999999999999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6799999999999999\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.700053421427285e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.700053421427285e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.626169212347464e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.626169212347464e-05\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[1]\tvalid_0's multi_logloss: 0.849007\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.741751\n",
      "[3]\tvalid_0's multi_logloss: 0.656732\n",
      "[4]\tvalid_0's multi_logloss: 0.583029\n",
      "[5]\tvalid_0's multi_logloss: 0.524833\n",
      "[6]\tvalid_0's multi_logloss: 0.47461\n",
      "[7]\tvalid_0's multi_logloss: 0.434246\n",
      "[8]\tvalid_0's multi_logloss: 0.399907\n",
      "[9]\tvalid_0's multi_logloss: 0.369401\n",
      "[10]\tvalid_0's multi_logloss: 0.344211\n",
      "[11]\tvalid_0's multi_logloss: 0.321235\n",
      "[12]\tvalid_0's multi_logloss: 0.302573\n",
      "[13]\tvalid_0's multi_logloss: 0.286174\n",
      "[14]\tvalid_0's multi_logloss: 0.271455\n",
      "[15]\tvalid_0's multi_logloss: 0.259988\n",
      "[16]\tvalid_0's multi_logloss: 0.24955\n",
      "[17]\tvalid_0's multi_logloss: 0.238998\n",
      "[18]\tvalid_0's multi_logloss: 0.22929\n",
      "[19]\tvalid_0's multi_logloss: 0.221607\n",
      "[20]\tvalid_0's multi_logloss: 0.216348\n",
      "[21]\tvalid_0's multi_logloss: 0.209388\n",
      "[22]\tvalid_0's multi_logloss: 0.203239\n",
      "[23]\tvalid_0's multi_logloss: 0.199277\n",
      "[24]\tvalid_0's multi_logloss: 0.195084\n",
      "[25]\tvalid_0's multi_logloss: 0.1916\n",
      "[26]\tvalid_0's multi_logloss: 0.187614\n",
      "[27]\tvalid_0's multi_logloss: 0.184948\n",
      "[28]\tvalid_0's multi_logloss: 0.182366\n",
      "[29]\tvalid_0's multi_logloss: 0.180577\n",
      "[30]\tvalid_0's multi_logloss: 0.17838\n",
      "[31]\tvalid_0's multi_logloss: 0.175831\n",
      "[32]\tvalid_0's multi_logloss: 0.174124\n",
      "[33]\tvalid_0's multi_logloss: 0.172042\n",
      "[34]\tvalid_0's multi_logloss: 0.171033\n",
      "[35]\tvalid_0's multi_logloss: 0.169881\n",
      "[36]\tvalid_0's multi_logloss: 0.168817\n",
      "[37]\tvalid_0's multi_logloss: 0.167537\n",
      "[38]\tvalid_0's multi_logloss: 0.166307\n",
      "[39]\tvalid_0's multi_logloss: 0.16527\n",
      "[40]\tvalid_0's multi_logloss: 0.164721\n",
      "[41]\tvalid_0's multi_logloss: 0.164288\n",
      "[42]\tvalid_0's multi_logloss: 0.163295\n",
      "[43]\tvalid_0's multi_logloss: 0.162401\n",
      "[44]\tvalid_0's multi_logloss: 0.161728\n",
      "[45]\tvalid_0's multi_logloss: 0.161057\n",
      "[46]\tvalid_0's multi_logloss: 0.16073\n",
      "[47]\tvalid_0's multi_logloss: 0.160424\n",
      "[48]\tvalid_0's multi_logloss: 0.16017\n",
      "[49]\tvalid_0's multi_logloss: 0.159759\n",
      "[50]\tvalid_0's multi_logloss: 0.15951\n",
      "[51]\tvalid_0's multi_logloss: 0.159325\n",
      "[52]\tvalid_0's multi_logloss: 0.158988\n",
      "[53]\tvalid_0's multi_logloss: 0.158714\n",
      "[54]\tvalid_0's multi_logloss: 0.158375\n",
      "[55]\tvalid_0's multi_logloss: 0.15794\n",
      "[56]\tvalid_0's multi_logloss: 0.15762\n",
      "[57]\tvalid_0's multi_logloss: 0.157366\n",
      "[58]\tvalid_0's multi_logloss: 0.157031\n",
      "[59]\tvalid_0's multi_logloss: 0.156804\n",
      "[60]\tvalid_0's multi_logloss: 0.156503\n",
      "[61]\tvalid_0's multi_logloss: 0.156245\n",
      "[62]\tvalid_0's multi_logloss: 0.15613\n",
      "[63]\tvalid_0's multi_logloss: 0.156002\n",
      "[64]\tvalid_0's multi_logloss: 0.155829\n",
      "[65]\tvalid_0's multi_logloss: 0.155679\n",
      "[66]\tvalid_0's multi_logloss: 0.155548\n",
      "[67]\tvalid_0's multi_logloss: 0.15542\n",
      "[68]\tvalid_0's multi_logloss: 0.155341\n",
      "[69]\tvalid_0's multi_logloss: 0.155226\n",
      "[70]\tvalid_0's multi_logloss: 0.155185\n",
      "[71]\tvalid_0's multi_logloss: 0.1551\n",
      "[72]\tvalid_0's multi_logloss: 0.15501\n",
      "[73]\tvalid_0's multi_logloss: 0.154891\n",
      "[74]\tvalid_0's multi_logloss: 0.154788\n",
      "[75]\tvalid_0's multi_logloss: 0.154732\n",
      "[76]\tvalid_0's multi_logloss: 0.154703\n",
      "[77]\tvalid_0's multi_logloss: 0.154667\n",
      "[78]\tvalid_0's multi_logloss: 0.154591\n",
      "[79]\tvalid_0's multi_logloss: 0.154562\n",
      "[80]\tvalid_0's multi_logloss: 0.154479\n",
      "[81]\tvalid_0's multi_logloss: 0.154477\n",
      "[82]\tvalid_0's multi_logloss: 0.154384\n",
      "[83]\tvalid_0's multi_logloss: 0.154397\n",
      "[84]\tvalid_0's multi_logloss: 0.154356\n",
      "[85]\tvalid_0's multi_logloss: 0.154374\n",
      "[86]\tvalid_0's multi_logloss: 0.15437\n",
      "[87]\tvalid_0's multi_logloss: 0.15435\n",
      "[88]\tvalid_0's multi_logloss: 0.154345\n",
      "[89]\tvalid_0's multi_logloss: 0.154382\n",
      "[90]\tvalid_0's multi_logloss: 0.154367\n",
      "[91]\tvalid_0's multi_logloss: 0.154335\n",
      "[92]\tvalid_0's multi_logloss: 0.154351\n",
      "[93]\tvalid_0's multi_logloss: 0.154352\n",
      "[94]\tvalid_0's multi_logloss: 0.154319\n",
      "[95]\tvalid_0's multi_logloss: 0.154354\n",
      "[96]\tvalid_0's multi_logloss: 0.154382\n",
      "[97]\tvalid_0's multi_logloss: 0.154388\n",
      "[98]\tvalid_0's multi_logloss: 0.154355\n",
      "[99]\tvalid_0's multi_logloss: 0.154394\n",
      "[100]\tvalid_0's multi_logloss: 0.154441\n",
      "[101]\tvalid_0's multi_logloss: 0.154491\n",
      "[102]\tvalid_0's multi_logloss: 0.154507\n",
      "[103]\tvalid_0's multi_logloss: 0.154548\n",
      "[104]\tvalid_0's multi_logloss: 0.154587\n",
      "Early stopping, best iteration is:\n",
      "[94]\tvalid_0's multi_logloss: 0.154319\n"
     ]
    }
   ],
   "source": [
    "p_val = np.zeros((trn.shape[0], n_class))\n",
    "p_tst = np.zeros((tst.shape[0], n_class))\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(trn, y), 1):\n",
    "    print(f'training model for CV #{i}')\n",
    "    clf = LGBMClassifier(**params)\n",
    "    clf.fit(trn[i_trn], y[i_trn],\n",
    "            eval_set=[(trn[i_val], y[i_val])],\n",
    "            eval_metric='multiclass',\n",
    "            early_stopping_rounds=10)\n",
    "    \n",
    "    p_val[i_val, :] = clf.predict_proba(trn[i_val])\n",
    "    p_tst += clf.predict_proba(tst) / n_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T07:09:33.218809Z",
     "start_time": "2020-10-06T07:09:32.438944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.4678%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'{accuracy_score(y, np.argmax(p_val, axis=1)) * 100:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T07:09:35.434253Z",
     "start_time": "2020-10-06T07:09:34.705340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320000, 3) (80000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(p_val.shape, p_tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T07:09:39.764236Z",
     "start_time": "2020-10-06T07:09:36.102319Z"
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt(p_val_file, p_val, fmt='%.6f', delimiter=',')\n",
    "np.savetxt(p_tst_file, p_tst, fmt='%.6f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T07:09:41.754507Z",
     "start_time": "2020-10-06T07:09:40.306917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='feature'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imp = pd.DataFrame({'feature': df.columns, 'importance': clf.feature_importances_})\n",
    "imp = imp.sort_values('importance').set_index('feature')\n",
    "imp.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T07:09:42.607016Z",
     "start_time": "2020-10-06T07:09:41.760501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320000</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320001</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320002</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320003</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320004</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        class\n",
       "id           \n",
       "320000      0\n",
       "320001      0\n",
       "320002      0\n",
       "320003      0\n",
       "320004      0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sub = pd.read_csv(sample_file, index_col=0)\n",
    "print(sub.shape)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T07:09:44.134617Z",
     "start_time": "2020-10-06T07:09:43.173972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320000</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320001</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320002</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320003</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320004</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        class\n",
       "id           \n",
       "320000      2\n",
       "320001      0\n",
       "320002      2\n",
       "320003      0\n",
       "320004      2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub[target_col] = np.argmax(p_tst, axis=1)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T07:09:45.276001Z",
     "start_time": "2020-10-06T07:09:44.537424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    40725\n",
       "0    29983\n",
       "1     9292\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sub[target_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T07:09:46.589009Z",
     "start_time": "2020-10-06T07:09:45.664481Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "sub.to_csv(sub_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
